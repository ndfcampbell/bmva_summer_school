{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Grad Example (with Scalars)\n",
    "\n",
    "## or \"Find out how PyTorch work behind the scenes!\"\n",
    "\n",
    "*Neill Campbell and Tom Haines, University of Bath, 2023*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Read \"_How to auto grad_\" below, to understand the basic concept.\n",
    "2. Read \"_Solution structure_\" below, to understand how the version provided works.\n",
    "3. Understand the code that has been provided\n",
    "4. Extend it so you have the  operations needed for linear regression\n",
    "5. Implement linear regression with gradient descent using your auto grad system. Plot the solution and data on one graph and the history of the loss on another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you should learn\n",
    "\n",
    "This is about understanding how an automatic differentiation system, such as PyTorch or TensorFlow, works.\n",
    "It's much more of a tutorial than the previous lab, and shouldn't take too long.\n",
    "While this may seem somewhat pointless, given the existence of these libraries, the reality is (as you may have already discovered) that it's very easy to write code with them that you think will work, only to discover that it doesn't.\n",
    "By implementing your own, as simple as it may be, you'll understand internally what such libraries are doing and hence how they can go wrong.\n",
    "This will make you much better at using these libraries to solve real problems.\n",
    "It will also give you the understanding you need for if you ever need to extend such a library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to auto grad\n",
    "\n",
    "Automatic differentiation is just the iterated application of the chain rule: if I have `a = op(b)` for any operation `op` and I know the derivative of `a` with respect to the loss of my objective ($\\frac{d\\text{loss}}{da}$) and I know the derivative of `op`, ($\\frac{da}{db}$) then the chain rule gives us\n",
    "\n",
    "$$\\frac{d\\text{loss}}{db} = \\frac{d\\text{loss}}{da}\\frac{da}{db}$$\n",
    "\n",
    "By doing this repeatedly we can work backwards, from the final calculation of the loss function to any parameter that went into our model.\n",
    "\n",
    "To do this we need a _computation graph_, i.e. as we perform operations we don't just do what what has been asked of us but also keep a record, a graph of the operations that got us to where we currently are.\n",
    "The chain rule can hence be used to propagate gradients back throughout this graph (it could be a tree, and that might be easier way to think about it in the first instance, but typically parts get used more than once (e.g. parameters once per data point), turning it into a graph).\n",
    "\n",
    "This means any auto grad system works in two phases: a forwards pass in which the actual value is calculated, then a backwards pass where gradients are pushed backwards (backprop) to the parameters that we then optimise with gradient descent or a friend.\n",
    "For the presented system (and PyTorch) the forward pass happens as the lines of code are run, as for normal code; the backwards pass happens when you call `backward()` on your loss function.\n",
    "This is a recursive function that calls back through the computation graph until it gets to the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution structure\n",
    "\n",
    "Firstly, it's impossible to do this sensibly without objects, so this is going to involve object oriented programming.\n",
    "I've kept it as a simple as possible, so hopefully most of it should just make sense.\n",
    "\n",
    "The key trick is to have an object rigged so that when you perform standard mathematical operations it instead generates the relevant part of the computation graph and returns that instead.\n",
    "This is `Scalar` below; it also calculates and stores the computed value as the graph is constructed.\n",
    "\n",
    "The computation graph is simply a set of nodes that represent mathematical operations, for instance addition, that point to the nodes that they have added together.\n",
    "These other nodes could be values, further addition nodes or any other mathematical operation.\n",
    "For the backwards pass each `Node` needs to have the ability to calculate the gradient of it's output with respect to any input so it can then multiply that with the gradient given to it and push that backwards in the graph.\n",
    "Below there is a parent class, `Node`, with a standard interface that all nodes within the computation graph must provide.\n",
    "There is then a child class for every operation, plus the `Value` class to support actual values.\n",
    "\n",
    "One way of doing this is to have some kind of mega class that does all of the operations then have a variant for each mathematical operation.\n",
    "This gets kinda nasty.\n",
    "A better approach is to separate out the computation graph from the interface; this has been done below.\n",
    "`Node` and its children are the computation graph while `Scalar` is the interface.\n",
    "You actually end up with a bipartite graph in memory: `Scalar` objects always contain a reference to a `Node` object and `Node` objects only contain references to `Scalar` objects.\n",
    "\n",
    "To really understand the provided code you're going to have to read it…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "* You will need to implement addition (`__add__`), subtraction (`__sub__`) and multiplication (`__mul__`) to be able to do linear regression. The terms in brackets are the special methods required to override these operations for a class.\n",
    "* You don't need the `exp` operation, it's just there as an example, so you can see a unary operator alongside the binary operator that is `power` (`**`).\n",
    "* You may decide to implement a `Sum` node alongside a `sum()` function, as you do have to sum over the data set when calculating the loss function. But it's just as easy to do the loop.\n",
    "* You don't have operators such as +=, and don't try to implement them: they aren't actually possible, at least not simply, and the reason why is quite subtle. Just use $a = a + b$ instead.\n",
    "* Implementing linear regression is super simple but has the potential to bite you: unlike systems such as PyTorch there is far less safety here and it could quite happily go horribly wrong. Take particular care to use `detach()` as needed. This may be frustrating, but it's also where you will learn the most.\n",
    "* Beware `zero_grad()`. It has different semantics to PyTorch and a horrendously evil moustache.\n",
    "* For the linear regression I initialised both $m$ and $c$ to be 0: 1024 steps with a step size of 1e-3 converged well enough. Standard squared loss with standard gradient descent — don't need anything complicated here.\n",
    "* In a real system you have the ability to indicate which nodes need gradients calculated and which do not, for reasons of efficiency (`requires_grad` in PyTorch). This has been dropped here for simplicity. Yes, you will be calculating the gradient of your step size with respect to your loss function, whatever that means!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto grad system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"A node of the computation graph - mostly an interface with little functionality.\n",
    "    All nodes inherit from it.\"\"\"\n",
    "    \n",
    "    def kind(self):\n",
    "        \"\"\"Returns a string identifying the operation or value this node represents.\n",
    "        Not functionally needed, but a useful representation to have.\"\"\"\n",
    "        return 'node'\n",
    "    \n",
    "    def children(self):\n",
    "        \"\"\"Returns a list of the immediate children of this Node in the computation graph.\n",
    "        These will be Scalar objects, as they need to be wrapped for it to work.\n",
    "        Not needed to make it work, but allows for visualisation and avoids the\n",
    "        need to implement zero_grad() for every kind of node.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluates the computation graph at this point, returning the result.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zeros the gradient of this node and every child within the computation graph.\"\"\"\n",
    "        # Just need to pass the call back; we have the ability to get all of the\n",
    "        # children, so use it!..\n",
    "        for child in self.children():\n",
    "            child.zero_grad()\n",
    "    \n",
    "    def backward(self, grad=1):\n",
    "        \"\"\"Passes the gradient backwards using the chain rule.\"\"\"\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(Node):\n",
    "    \"\"\"Represents a single value.\"\"\"\n",
    "    \n",
    "    def __init__(self, value):\n",
    "        self._value = float(value)\n",
    "    \n",
    "    def kind(self):\n",
    "        return 'value'\n",
    "    \n",
    "    def children(self):\n",
    "        return []\n",
    "    \n",
    "    def evaluate(self):\n",
    "        return self._value\n",
    "    \n",
    "    def backward(self, grad=1):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp(Node):\n",
    "    \"\"\"Calculates the exponent of its parameter.\"\"\"\n",
    "    \n",
    "    def __init__(self, param):\n",
    "        assert isinstance(param, Scalar)\n",
    "        self._param = param\n",
    "    \n",
    "    def kind(self):\n",
    "        return 'exp'\n",
    "    \n",
    "    def children(self):\n",
    "        return [self._param]\n",
    "    \n",
    "    def evaluate(self):\n",
    "        return numpy.exp(self._param.value)\n",
    "    \n",
    "    def backward(self, grad=1):\n",
    "        self._param.backward(grad * numpy.exp(self._param.value))\n",
    "\n",
    "\n",
    "# Nobody want to use the above class directly, hence having a nice wrapper function that\n",
    "# behaves identically to the normal one...\n",
    "# (you only do this for operations that Scalar can't automatically sneak in)\n",
    "def exp(value):\n",
    "    if not isinstance(value, Scalar):\n",
    "        value = Scalar(value)\n",
    "    return Scalar(Exp(value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Power(Node):\n",
    "    \"\"\"Calculates the first parameter to the power of the second parameter.\"\"\"\n",
    "    \n",
    "    def __init__(self, lhs, rhs):\n",
    "        \"\"\"lhs = left hand side, rhs = right hand side; represents lhs ** rhs\"\"\"\n",
    "        assert isinstance(lhs, Scalar)\n",
    "        assert isinstance(rhs, Scalar)\n",
    "        \n",
    "        self._lhs = lhs\n",
    "        self._rhs = rhs\n",
    "    \n",
    "    def kind(self):\n",
    "        return '^'\n",
    "    \n",
    "    def children(self):\n",
    "        return [self._lhs, self._rhs]\n",
    "    \n",
    "    def evaluate(self):\n",
    "        return self._lhs.value ** self._rhs.value\n",
    "    \n",
    "    def backward(self, grad=1):\n",
    "        self._lhs.backward(grad * self._rhs.value * self._lhs.value ** (self._rhs.value-1))\n",
    "        \n",
    "        # This is only defined, at least without using complex numbers, for the lhs being\n",
    "        # positive; if it isn't give up and break the computation (wrong answer will be generated)...\n",
    "        # (max is for safety, to avoid numerical overflow)\n",
    "        if self._lhs.value>0.0:\n",
    "            d_rhs = grad * self._lhs.value ** self._rhs.value * numpy.log(max(self._lhs.value, 1e-256))\n",
    "            self._rhs.backward(d_rhs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    \"\"\"Adds two numbers together.\"\"\"\n",
    "    \n",
    "    def __init__(self, lhs, rhs):\n",
    "        \"\"\"lhs = left hand side, rhs = right hand side; represents lhs + rhs\"\"\"\n",
    "        assert isinstance(lhs, Scalar)\n",
    "        assert isinstance(rhs, Scalar)\n",
    "        \n",
    "        self._lhs = lhs\n",
    "        self._rhs = rhs\n",
    "    \n",
    "    def kind(self):\n",
    "        return '+'\n",
    "    \n",
    "    def children(self):\n",
    "        return [self._lhs, self._rhs]\n",
    "    \n",
    "    def evaluate(self):\n",
    "        return self._lhs.value + self._rhs.value\n",
    "    \n",
    "    def backward(self, grad=1):\n",
    "        self._lhs.backward(grad)\n",
    "        self._rhs.backward(grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sub(Node):\n",
    "    \"\"\"Subtracts the second number from the first.\"\"\"\n",
    "    \n",
    "    def __init__(self, lhs, rhs):\n",
    "        \"\"\"lhs = left hand side, rhs = right hand side; represents lhs - rhs\"\"\"\n",
    "        assert isinstance(lhs, Scalar)\n",
    "        assert isinstance(rhs, Scalar)\n",
    "        \n",
    "        self._lhs = lhs\n",
    "        self._rhs = rhs\n",
    "    \n",
    "    def kind(self):\n",
    "        return '-'\n",
    "    \n",
    "    def children(self):\n",
    "        return [self._lhs, self._rhs]\n",
    "    \n",
    "    def evaluate(self):\n",
    "        return self._lhs.value - self._rhs.value\n",
    "    \n",
    "    def backward(self, grad=1):\n",
    "        self._lhs.backward(grad)\n",
    "        self._rhs.backward(-grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sum(Node):\n",
    "    \"\"\"This is entirely unnecessary, but really rather convienient given that summing over\n",
    "    a data set is a common activity. Sums of lots of numbers as a single Node.\"\"\"\n",
    "    \n",
    "    def __init__(self, terms):\n",
    "        for term in terms:\n",
    "            assert isinstance(term, Scalar)\n",
    "\n",
    "        self._terms = list(terms)\n",
    "        \n",
    "    def kind(self):\n",
    "        return 'sum'\n",
    "    \n",
    "    def children(self):\n",
    "        return self._terms\n",
    "    \n",
    "    def evaluate(self):\n",
    "        ret = 0.0\n",
    "        for term in self._terms:\n",
    "            ret += term.value\n",
    "        return ret\n",
    "    \n",
    "    def backward(self, grad=1):\n",
    "        for term in self._terms:\n",
    "            term.backward(grad)\n",
    "        \n",
    "\n",
    "def sum(terms):\n",
    "    return Scalar(Sum([(term if isinstance(term, Scalar) else Scalar(term)) for term in terms]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mult(Node):\n",
    "    \"\"\"Multiplies two numbers together.\"\"\"\n",
    "    \n",
    "    def __init__(self, lhs, rhs):\n",
    "        \"\"\"lhs = left hand side, rhs = right hand side; represents lhs * rhs\"\"\"\n",
    "        assert isinstance(lhs, Scalar)\n",
    "        assert isinstance(rhs, Scalar)\n",
    "        \n",
    "        self._lhs = lhs\n",
    "        self._rhs = rhs\n",
    "    \n",
    "    def kind(self):\n",
    "        return '*'\n",
    "    \n",
    "    def children(self):\n",
    "        return [self._lhs, self._rhs]\n",
    "    \n",
    "    def evaluate(self):\n",
    "        return self._lhs.value * self._rhs.value\n",
    "    \n",
    "    def backward(self, grad=1):\n",
    "        \n",
    "        ### ENTER CODE TO PERFORM THE BACKWARDS PASS USING THE JACOBIAN OF THE MULTIPLY FUNCTION (PRODUCT RULE) ###\n",
    "        \n",
    "        raise NotImplementedError('Need to enter code to perform the multiply backwards function!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Div(Node):\n",
    "    \"\"\"Divides the first number by the second.\"\"\"\n",
    "    \n",
    "    def __init__(self, lhs, rhs):\n",
    "        \"\"\"lhs = left hand side, rhs = right hand side; represents lhs / rhs\"\"\"\n",
    "        assert isinstance(lhs, Scalar)\n",
    "        assert isinstance(rhs, Scalar)\n",
    "        \n",
    "        self._lhs = lhs\n",
    "        self._rhs = rhs\n",
    "    \n",
    "    def kind(self):\n",
    "        return '/'\n",
    "    \n",
    "    def children(self):\n",
    "        return [self._lhs, self._rhs]\n",
    "    \n",
    "    def evaluate(self):\n",
    "        return self._lhs.value / self._rhs.value\n",
    "    \n",
    "    def backward(self, grad=1):\n",
    "        \n",
    "        ### ENTER CODE TO PERFORM THE BACKWARDS PASS USING THE JACOBIAN OF THE DIVIDE FUNCTION (QUOTIENT RULE) ###\n",
    "        \n",
    "        raise NotImplementedError('Need to enter code to perform the divide backwards function!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    \"\"\"This is a conveniance wrapper around nodes, that exists so everything behaves sensibly.\n",
    "    In PyTorch this is exactly equivalent to the Tensor class, except it only does one value at\n",
    "    a time, rather than an nD array, hence the name Scalar, because that's all it represents.\n",
    "    It's main purpose is to override operations, so when you do maths on it the\n",
    "    right type of node is created and it returns a new scalar with the expected\n",
    "    computation graph.\"\"\"\n",
    "    \n",
    "    def __init__(self, node, name = None):\n",
    "        \"\"\"Lets you create a scalar, either from a number or from a Node. You can also provide\n",
    "        a name to communicate what the Scalar represents.\"\"\"\n",
    "        # As a matter of conveniance, and to copy PyTorch, if you pass in a number it is converted...\n",
    "        if isinstance(node, numbers.Number):\n",
    "            node = Value(node)\n",
    "        \n",
    "        assert isinstance(node, Node)\n",
    "        \n",
    "        self._node = node\n",
    "        self._name = name\n",
    "        \n",
    "        self._value = node.evaluate()\n",
    "        self._grad = 0.0\n",
    "    \n",
    "    \n",
    "    @property # @property decorator means this acts as a read only variable; good for safety\n",
    "    def kind(self):\n",
    "        \"\"\"Returns the kind of operation it represents; informational.\"\"\"\n",
    "        return self._node.kind()\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"If a name has been set this provides it; None if a name has not been set.\"\"\"\n",
    "        return self._name\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        \"\"\"Returns the value of the Scalar.\"\"\"\n",
    "        return self._value\n",
    "    \n",
    "    @property\n",
    "    def grad(self):\n",
    "        \"\"\"Returns the gradient of the Scalar.\"\"\"\n",
    "        return self._grad\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def node(self):\n",
    "        \"\"\"Gives you access to the computation graph wrapped by the Scalar. Not\n",
    "        needed for normal use.\"\"\"\n",
    "        return self._node\n",
    "    \n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zeroes all gradients in the computation graph; typically used at the start of\n",
    "        each step of optimisation right before backward is called. Note that this operates\n",
    "        differently to the approach in PyTorch, which is the better approach. This is\n",
    "        inefficient and has a nasty gotcha: it's easy to accidentally call it when you're\n",
    "        parameters are not part of the computation graph.\"\"\"\n",
    "        self._grad = 0.0\n",
    "        self._node.zero_grad()\n",
    "    \n",
    "    \n",
    "    def backward(self, grad=1):\n",
    "        \"\"\"Sends a gradient backwards through the computatiuon graph, updating all values with\n",
    "        respect to the gradient here. The parameter defaults to 1, because the gradient of a\n",
    "        variable with respect to itself is 1, and this is typically called on just the final\n",
    "        loss to push back to all parameters their gradient with respect to the loss.\"\"\"\n",
    "        \n",
    "        # Update this nodes record of its gradient; it's += because the computation graph\n",
    "        # will typically reach the same node through multiple routes...\n",
    "        self._grad += grad\n",
    "        \n",
    "        # Continue to send the gradient backwards, to update all of the graph...\n",
    "        self._node.backward(grad)\n",
    "    \n",
    "    \n",
    "    def detach(self):\n",
    "        \"\"\"The problem with computation graphs is that they just keep growing, until you\n",
    "        run out of memory. For this reason care has to be taken to cull them back, before\n",
    "        they overwhelm you. The detach operation does exactly that: it detaches from the\n",
    "        computation graph that feeds into this Scalar, by replacing the Node with the\n",
    "        dead end that is a Value().\"\"\"\n",
    "        self._node = Value(self._value)\n",
    "    \n",
    "    \n",
    "    def __add__(self, rhs):\n",
    "        if not isinstance(rhs, Scalar):\n",
    "            rhs = Scalar(rhs)\n",
    "        return Scalar(Add(self, rhs))\n",
    "    \n",
    "    def __sub__(self, rhs):\n",
    "        if not isinstance(rhs, Scalar):\n",
    "            rhs = Scalar(rhs)\n",
    "        return Scalar(Sub(self, rhs))\n",
    "    \n",
    "    def __mul__(self, rhs):\n",
    "        if not isinstance(rhs, Scalar):\n",
    "            rhs = Scalar(rhs)\n",
    "        return Scalar(Mult(self, rhs))\n",
    "    \n",
    "    def __truediv__(self, rhs):\n",
    "        if not isinstance(rhs, Scalar):\n",
    "            rhs = Scalar(rhs)\n",
    "        return Scalar(Div(self, rhs))\n",
    "    \n",
    "    def __pow__(self, rhs):\n",
    "        \"\"\"This method gets called when you call self**rhs and self is an instance of this class.\"\"\"\n",
    "        if not isinstance(rhs, Scalar):\n",
    "            rhs = Scalar(rhs)\n",
    "        return Scalar(Power(self, rhs))\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"This is what you get when you print this object.\"\"\"\n",
    "        return f'Scalar(value={self.value:.6f}, grad={self.grad:.6f}, kind={self.kind}, name={self.name})'\n",
    "    \n",
    "    \n",
    "    def equation(self, use_names = True):\n",
    "        \"\"\"The computation graph is rather nice in that you can use it to generate an equation\n",
    "        for what has been computed. You can ignore this code, but this method can be useful\n",
    "        (if far too bracket happy) for debugging.\"\"\"\n",
    "        \n",
    "        if isinstance(self._node, Value):\n",
    "            if self._name is not None and use_names:\n",
    "                return self._name\n",
    "            else:\n",
    "                return f'{self.value:.6f}'\n",
    "        \n",
    "        children = self._node.children()\n",
    "        if len(self.kind)==1 and len(children)==2:\n",
    "            # This formats binary operators as expected...\n",
    "            return f'({children[0].equation(use_names)}){self.kind}({children[1].equation(use_names)})'\n",
    "        \n",
    "        else:\n",
    "            # Everything else can look like a function...\n",
    "            return self.kind + '(' + ','.join([child.equation(use_names) for child in children]) + ')'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of a value with respect to itself better be 1!..\n",
    "print('value:')\n",
    "a = Scalar(4)\n",
    "a.backward()\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "\n",
    "# Exponential is pretty straight forward...\n",
    "print('exp:')\n",
    "a = Scalar(4)\n",
    "b = exp(a)\n",
    "b.backward()\n",
    "print(a)\n",
    "print(b.equation())\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "\n",
    "# Power - gradients should be 4 and 2.77...\n",
    "print('power:')\n",
    "a = Scalar(2, 'a')\n",
    "b = Scalar(2, 'b')\n",
    "c = a**b\n",
    "c.backward()\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c.equation())\n",
    "print(c)\n",
    "print()\n",
    "\n",
    "\n",
    "# Gradients through a sum don't change...\n",
    "print('addition:')\n",
    "a = Scalar(2, 'a')\n",
    "b = Scalar(2, 'b')\n",
    "c = a + b\n",
    "c.backward()\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c.equation())\n",
    "print(c)\n",
    "print()\n",
    "\n",
    "\n",
    "# Subtraction will flip the gradient of the second term...\n",
    "print('subtraction:')\n",
    "a = Scalar(2, 'a')\n",
    "b = Scalar(2, 'b')\n",
    "c = a - b\n",
    "c.backward()\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c.equation())\n",
    "print(c)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in the Jacobian code for multiply and divide above!\n",
    "\n",
    "- **TODO:** The cell below will return \"NotImplemented\" exceptions until you have filled in the Jacobian calculations in the \"backward\" functions to propagate the gradients for multiply and divide in the \"Mult\" and \"Div\" classes above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication is more fun - gradient requires the product rule...\n",
    "print('multiplication:')\n",
    "a = Scalar(0.5, 'a')\n",
    "b = Scalar(2, 'b')\n",
    "c = a * b\n",
    "c.backward()\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c.equation())\n",
    "print(c)\n",
    "print()\n",
    "\n",
    "\n",
    "# Division - this requires the quotient rule - should get dc/da=0.25 and dc/db=-0.5...\n",
    "print('division:')\n",
    "a = Scalar(8, 'a')\n",
    "b = Scalar(4, 'b')\n",
    "c = a / b\n",
    "c.backward()\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c.equation())\n",
    "print(c)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression:\n",
    "\n",
    "\n",
    "- **TODO:** You need to fill in code to evaluate the loss function, find the gradient and update the parameters to perform gradient descent!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data - not a lot as this code is not very efficient...\n",
    "train_x = [1.0, 2.0, 3.0, 4.0]\n",
    "train_y = [2.2, 2.5, 2.4, 2.7]\n",
    "\n",
    "\n",
    "# Model is y = m*x + c, so we need the two parameters...\n",
    "m = Scalar(0.0, 'm')\n",
    "c = Scalar(0.0, 'c')\n",
    "\n",
    "# Step size; it will calculate the gradient for this, which is inefficient, \n",
    "# but this is only a demo so we can live with that. In practise you would\n",
    "# indicate which numbers you want gradients for any which you do not, for\n",
    "# efficency (see PyTorch's requires_grad)...\n",
    "step = Scalar(1e-3)\n",
    "\n",
    "\n",
    "### ENTER CODE TO ITERATIVELY OPTIMISE THE PARAMETERS ###\n",
    "raise NotImplementedError('Need to add code to perform gradient descent to learn parameters m and c.')\n",
    "\n",
    "\n",
    "# The equation...\n",
    "print(f'final loss = {history[-1]}')\n",
    "print(f'y = {m.value} x + {c.value}')\n",
    "\n",
    "\n",
    "# Plot the data and the solution...\n",
    "plt.figure(figsize=[6, 6])\n",
    "plt.scatter(train_x, train_y)\n",
    "plt.plot(train_x, [m.value * x + c.value for x in train_x])\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
