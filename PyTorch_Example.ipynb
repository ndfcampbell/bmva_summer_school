{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5y7O9fs7RvH",
        "outputId": "c28ed3a4-f4bc-4186-b6b5-ecb19cebc58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Collecting torch\n",
            "  Downloading torch-1.12.0-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.3 MB 18 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.13.0-cp37-cp37m-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.1 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "Collecting tensorboardcolab\n",
            "  Downloading tensorboardcolab-0.0.22.tar.gz (2.5 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Building wheels for collected packages: torchviz, tensorboardcolab\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4150 sha256=632f1d595698b42e0dd0ea07e402eca1baac31115ba9478d7f560ba13e2ccf48\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n",
            "  Building wheel for tensorboardcolab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorboardcolab: filename=tensorboardcolab-0.0.22-py3-none-any.whl size=3859 sha256=0b9531ba5bc6a638b909c0210efc169227693e46188ca235ea2b3a2a8e2fa927\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/4e/4a/1c6c267395cb10edded1050df12af165d3254cfce324e80941\n",
            "Successfully built torchviz tensorboardcolab\n",
            "Installing collected packages: torch, torchviz, torchvision, tensorboardcolab\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.12.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.12.0 which is incompatible.\n",
            "fastai 2.6.3 requires torch<1.12,>=1.7.0, but you have torch 1.12.0 which is incompatible.\u001b[0m\n",
            "Successfully installed tensorboardcolab-0.0.22 torch-1.12.0 torchvision-0.13.0 torchviz-0.0.2\n"
          ]
        }
      ],
      "source": [
        "# Standard numpy for matrices, vectors, etc..\n",
        "import numpy as np\n",
        "\n",
        "# Visualisation (plotting, etc..)\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "    in_colab = False\n",
        "    import google.colab\n",
        "    in_colab = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Use the following to access torch and tensorboard when running on colab\n",
        "if in_colab:\n",
        "    !pip install -U torch torchvision torchviz tensorboardcolab\n",
        "    from tensorboardcolab import *\n",
        "\n",
        "# New for today! Import PyTorch (refered to by package name torch)\n",
        "import torch\n",
        "\n",
        "# This is used for graph visualisation..\n",
        "from torchviz import make_dot\n",
        "\n",
        "# Please see the following page for getting \n",
        "# started guide and tutorials:\n",
        "# \u0016\u0016https://pytorch.org/tutorials/index.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ifdqeETHiV8z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9__IpqO7RvL",
        "outputId": "7f7c502e-a079-47cc-f05e-8c8411f83699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In numpy:\n",
            "a =  [1. 2. 3.]\n",
            "a =  [2. 2. 2.]\n",
            "a + b =  [3. 4. 5.]\n",
            "a ** b =  [1. 4. 9.]\n",
            "c = (a + b) * (a ** b) =  [ 3. 16. 45.]\n"
          ]
        }
      ],
      "source": [
        "# What we are used to in standard nummerical programming:\n",
        "\n",
        "a = np.array([1.0, 2.0, 3.0])\n",
        "\n",
        "b = np.array([2.0, 2.0, 2.0])\n",
        "\n",
        "a_plus_b = a + b\n",
        "\n",
        "a_power_b = a ** b\n",
        "\n",
        "c = a_plus_b * a_power_b\n",
        "\n",
        "print('In numpy:')\n",
        "print('a = ', a)\n",
        "print('a = ', b)\n",
        "print('a + b = ', a_plus_b)\n",
        "print('a ** b = ', a_power_b)\n",
        "print('c = (a + b) * (a ** b) = ', c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksIh9vuA7RvO",
        "outputId": "70fb92b1-4640-4df9-a808-335444e230f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In torch:\n",
            "a =  tensor([1., 2., 3.], dtype=torch.float64)\n",
            "a =  tensor([2., 2., 2.], dtype=torch.float64)\n",
            "a + b =  tensor([3., 4., 5.], dtype=torch.float64)\n",
            "a ** b =  tensor([1., 4., 9.], dtype=torch.float64)\n",
            "c = (a + b) * (a ** b) =  tensor([ 3., 16., 45.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# Let's do this with pytorch (or just torch):\n",
        "#\n",
        "# (Using t_ to denote torch variables - this is optional and just for clarity)\n",
        "#\n",
        "\n",
        "t_a = torch.tensor(a)\n",
        "\n",
        "t_b = torch.tensor(b)\n",
        "\n",
        "t_a_plus_b = t_a + t_b\n",
        "\n",
        "t_a_power_b = t_a ** t_b\n",
        "\n",
        "t_c = t_a_plus_b * t_a_power_b\n",
        "\n",
        "print('In torch:')\n",
        "print('a = ', t_a)\n",
        "print('a = ', t_b)\n",
        "print('a + b = ', t_a_plus_b)\n",
        "print('a ** b = ', t_a_power_b)\n",
        "print('c = (a + b) * (a ** b) = ', t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So why would we use torch instead of numpy?\n",
        "\n",
        "So far, the two seem to operate in the same manner so why do we need torch?\n",
        "\n",
        "In the background, torch is also builds a computational graph of the operations being performed. This will allow, amongst other things, the graph to be analysed and gradients to be computed automatically by going backwards through the graph applying the differentiation **chain rule** to propagate gradient information.\n",
        "\n",
        "*Let's see an example in action!*\n",
        "\n",
        "#### Aside: More information about pytorch autograd:\n",
        "\n",
        "- [Automatic Differentiation with torch.autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
        "\n",
        "- [Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)"
      ],
      "metadata": {
        "id": "hcTzKahK8W22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do the same calculation as before but we will tell torch that we would\n",
        "# like to calculate gradients by using the \"requires_grad=True\" argument when\n",
        "# we create the pytorch tensors..\n",
        "\n",
        "t_a = torch.tensor(a, requires_grad=True)\n",
        "\n",
        "t_b = torch.tensor(b, requires_grad=True)\n",
        "\n",
        "t_a_plus_b = t_a + t_b\n",
        "\n",
        "t_a_power_b = t_a ** t_b\n",
        "\n",
        "t_c = t_a_plus_b * t_a_power_b\n",
        "\n",
        "print('t_c = ', t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhgNc5eE8PDf",
        "outputId": "99ad9092-c9a0-407f-9edf-951b40c0fd2a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t_c =  tensor([ 3., 16., 45.], dtype=torch.float64, grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We notice t_c has a new attribute!\n",
        "\n",
        "If we look the printed output for t_c we notice there is a new piece of information, now t_c has an attribute called \"grad_fn\". \n",
        "\n",
        "This indicates that there is a function associated with the tensor to propagate the gradient backward (part of the *computational graph* we mentioned before).\n",
        "\n",
        "We notice that the grad_fn object is of type **MulBackward** which indicates that this is a function that calculates the gradient through a multiplication operation. We remember that t_c is the result of a multiplication:\n",
        "`t_c = t_a_plus_b * t_a_power_b`\n",
        "so we would expect the gradient to require an application of the chain rule via the derivative of a multiplication operation.\n"
      ],
      "metadata": {
        "id": "ErYHlyY8AZN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can visualise the computational graph for all the relevant terms:\n",
        "\n",
        "print('t_c gradient function = ', t_c.grad_fn)\n",
        "print('t_a_plus_b gradient function = ', t_a_plus_b.grad_fn)\n",
        "print('t_a_power_b gradient function = ', t_a_power_b.grad_fn)\n",
        "print('t_b gradient function = ', t_b.grad_fn)\n",
        "print('t_a gradient function = ', t_a.grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgRPajy18PY8",
        "outputId": "a4eba487-b020-4527-b84c-1ca962354579"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t_c gradient function =  <MulBackward0 object at 0x7fa40e586b50>\n",
            "t_a_plus_b gradient function =  <AddBackward0 object at 0x7fa40e5772d0>\n",
            "t_a_power_b gradient function =  <PowBackward1 object at 0x7fa40e577090>\n",
            "t_b gradient function =  None\n",
            "t_a gradient function =  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We note that when we get to t_a and t_b we no longer have any gradient functions since these are the starting points of the calculation (and not dependent on any other values). \n",
        "\n",
        "Otherwise, we can see that a gradient function is associated with each operation using a suitable function. So t_a_plus_b has **AddBackward** since it is the result of an addition operation. Equally, t_a_power_b has **PowBackward** since it is the result of taking a to the power of b.\n",
        "\n",
        "**We can now see these gradient functions in action!** We will now make use of the gradient functions by running the *backward* operation in torch which tells the computational graph to propagate gradients.\n",
        "\n",
        "Let us calculate the gradients of sum(t_c) wrt t_a and t_b. That is we want\n",
        "$$\n",
        "\\frac{d s}{d \\mathbf{a}} \\quad \\text{and} \\quad \\frac{d s}{d \\mathbf{b}}\n",
        "$$\n",
        "where $s = \\sum_i c_i$."
      ],
      "metadata": {
        "id": "USrUraRQBvzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a result to hold the scalar sum\n",
        "t_s = torch.sum(t_c)\n",
        "\n",
        "# Propagate gradients for t_s by calling backward..\n",
        "t_s.backward()\n",
        "\n",
        "# Now we can read out the gradients..\n",
        "print('Gradient for t_a (ds/da) = ', t_a.grad)\n",
        "print('Gradient for t_b (ds/db) = ', t_b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8xNGhnJCuTr",
        "outputId": "dc960402-458f-438e-9941-56c46c257203"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient for t_a (ds/da) =  tensor([ 7., 20., 39.], dtype=torch.float64)\n",
            "Gradient for t_b (ds/db) =  tensor([ 1.0000, 15.0904, 58.4376], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check that these results make sense.\n",
        "\n",
        "We have the following:\n",
        "$$\n",
        "s = \\sum_i c_i, \\quad c_i = (a_i + b_i) \\times {a_i}^{b_i}\n",
        "$$\n",
        "\n",
        "Now if we remember our calculus rules, we will need to apply the following identities:\n",
        "$$\n",
        "\\frac{d}{dx} (a+b) = \\frac{da}{dx} + \\frac{db}{dx}\n",
        "$$\n",
        "$$\n",
        "\\frac{d}{dx} (a \\times b) = \\frac{da}{dx} \\times b + a \\times \\frac{db}{dx}\n",
        "$$\n",
        "$$\n",
        "\\frac{d}{dx} (x^a) = a \\times x^{a - 1}\n",
        "$$\n",
        "The final exponent rule (needed for t_b) is a little more involved:\n",
        "$$\n",
        "\\frac{d}{dx} (a^b) = \n",
        "\\frac{d}{dx} (e^{b \\ln a}) =\n",
        "(e^{b \\ln a}) \\frac{d}{dx} (b \\ln a) = \n",
        "a^b \\left( \\frac{db}{dx} \\ln a + \\frac{da}{dx} \\frac{b}{a} \\right)\n",
        "$$\n",
        "\n",
        "So, we can mimic the work of torch for t_a:\n",
        "\n",
        "$$\n",
        "\\frac{d c_i}{da_i}\n",
        "  = \\frac{d}{da_i} [(a_i + b_i) \\times {a_i}^{b_i}]\n",
        "  = {a_i}^{b_i} \\times \\left[\\frac{d}{da_i} (a_i + b_i)\\right]\n",
        "  + (a_i + b_i) \\times \\left[\\frac{d}{da_i} ({a_i}^{b_i}) \\right]\n",
        "$$\n",
        "$$\n",
        "\\Rightarrow \\frac{d c_i}{da_i} \n",
        "  = {a_i}^{b_i} + (a_i + b_i) \\times (b_i \\times {a_i}^{b_i - 1})\n",
        "$$\n",
        "\n",
        "**Let's check the pytorch result..**"
      ],
      "metadata": {
        "id": "aqTk4s7xFeKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate our hand-derived gradient for t_a..\n",
        "\n",
        "t_dc_da = (t_a ** t_b) + (t_a + t_b) * t_b * (t_a ** (t_b - 1.0))\n",
        "\n",
        "print('Check t_dc_da = ', t_dc_da)\n",
        "\n",
        "print('Torch Gradient for t_a = ', t_a.grad)\n",
        "\n",
        "# Hopefully these two match!!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G4XmmLpCvCE",
        "outputId": "5a4cefe5-84c8-4c88-e538-6158ac7d23de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check t_dc_da =  tensor([ 7., 20., 39.], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "Torch Gradient for t_a =  tensor([ 7., 20., 39.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVPXgcGr7RvX"
      },
      "source": [
        "### So it seems to work!\n",
        "\n",
        "If you would like you can check the result for t_b as well..\n",
        "\n",
        "Note: This is a more involved derivation (due to the more complex exponential rule); we can already see that even for our simple calculation, the gradient derivations can get very involved and so hand-calculation is error prone, time consuming and has to be changed with every modification to the original calculation. *The automatic differentiation in torch is really helping out!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0wunnT57RvY"
      },
      "source": [
        "### Why do we need to calculate derivations? \n",
        "\n",
        "Well, what if we are doing an optimistion? Let's see a full motivating example..\n",
        "\n",
        "## Example: fitting the parameters of a distribution\n",
        "\n",
        "Suppose we want to fit a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ to a set of numbers $X = \\{ x_0, x_1, \\dots, x_{N-1} \\}$.\n",
        "\n",
        "**We know that this fit can actually be calculated directly (one of the nice properties of the Gaussian distribution) but we will pretend that we cannot determine these parameters in closed-form and will perform nummerical optimisation to determine them - this way we can check our results with the analytic solution!**\n",
        "\n",
        "If we assume the numbers are i.i.d. (indentically and independently distributed) samples from a Gaussian then the likelihood of $X$ is given by:\n",
        "\n",
        "\\begin{align}\n",
        "p(X) &= p(x_0) \\cdot p(x_1) \\cdot \\dots \\cdot p(x_{N-1}) \\\\\n",
        " &= \\mathcal{N}(x_0 \\,|\\, \\mu, \\sigma^2) \\cdot \\mathcal{N}(x_1 \\,|\\, \\mu, \\sigma^2) \\cdot \\dots \\cdot \\mathcal{N}(x_{N-1} \\,|\\, \\mu, \\sigma^2) \\\\\n",
        " &= \\prod_{n=0}^{N-1} \\mathcal{N}(x_{n} \\,|\\, \\mu, \\sigma^2) \\\\\n",
        " &= \\prod_{n=0}^{N-1} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \n",
        "    \\exp{\\left( - \\frac{(x_{n} - \\mu)^2}{2\\sigma^2} \\right)}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP90UPyS7RvY"
      },
      "source": [
        "**Top Tip!** When working with exponential family of distributions it often helps to work in the log domain..\n",
        "\n",
        "\\begin{align}\n",
        "\\log  p(X)  &= \\sum_{n=0}^{N-1} \n",
        "    -\\frac{1}{2} \\log{\\left( 2\\pi\\sigma^2 \\right)}\n",
        "    -\\frac{(x_{n} - \\mu)^2}{2\\sigma^2}\n",
        "\\end{align}\n",
        "\n",
        "So, we have the *maximum likelihood* fit to the parameters when we find the values of $\\mu$ and $\\sigma^2$ that maximise $p(X)$ which (since $\\log\\,(\\cdot)$ is a concave function) occurs at the same time that $\\log p(X)$ is maximised."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "573itA7E7RvY"
      },
      "source": [
        "In our case we can find an analytic solution for\n",
        "\n",
        "\\begin{align}\n",
        "\\mu^{*} &= {\\arg\\max}_{\\mu} \\, \\log p(X) \\\\\n",
        "{\\sigma^{*}}^2 &= {\\arg\\max}_{\\sigma^2} \\, \\log p(X) \\\\\n",
        "\\end{align}\n",
        "\n",
        "But let's pretend that the problem was more complicated and we needed to use *optimisation* to solve the problem.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuD00Gg97RvZ"
      },
      "source": [
        "To perform numerical optimisation we need to be able to calculate gradients of the objective function ($\\log p(X)$) wrt the parameters that you are optimising ($\\mu$ and $\\sigma^2$).\n",
        "\n",
        "*Let's see how to do this in torch..*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8VYhQuth7RvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282ae027-c843-4dcb-e5eb-d8b65067d52a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X = \n",
            " [2.905 3.288 0.541 1.168 1.468 2.697 1.98  1.288\n",
            " 2.493 3.846 3.542 5.499 1.966 0.952 0.525 1.998\n",
            " 5.414 0.407 1.147 3.005]\n"
          ]
        }
      ],
      "source": [
        "# First let's generate some numbers to fit the data to..\n",
        "\n",
        "# How many values of x?\n",
        "N = 20\n",
        "\n",
        "# Pick the real mean and variance..\n",
        "mu_true = 2.5\n",
        "sigma_true = 1.5\n",
        "\n",
        "x_n = np.random.normal(mu_true, sigma_true, N)\n",
        "\n",
        "np.set_printoptions(precision=3, linewidth=50)\n",
        "print('X = \\n', np.transpose(x_n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRMsdGeW7Rvb"
      },
      "source": [
        "We are now going to create our implicit torch computation graph but we are going to account for the fact that $\\mu$ and $\\sigma^2$ are no longer constants since we wish to vary their values to find the maximum of $\\log p(X)$. With numerical optimisation, we need to start with a guess for the values of $\\mu$ and $\\sigma^2$; in this case, we will start with\n",
        "\\begin{align}\n",
        "\\mu_{\\mathrm{initial}} &= 1\\\\\n",
        "\\sigma^2_{\\mathrm{initial}} &= 1\n",
        "\\end{align}\n",
        "\n",
        "**Top Tip!** Care needs to be taken with $\\sigma$ since it can only be a positive value (unlike $\\mu$ which can be any real number). In general, `torch` variables can be positive or negative. In this example we square the value of `t_sigma` before using it to ensure that `t_sigma_2` is a positive value but we shouldn't, therefore, use the value for `t_sigma` directly in calculations..\n",
        "\n",
        "As a reminder, we want to find:\n",
        "\\begin{align}\n",
        "\\log  p(X)  &= \\sum_{n=0}^{N-1} \n",
        "    -\\frac{1}{2} \\log{\\left( 2\\pi\\sigma^2 \\right)}\n",
        "    -\\frac{(x_{n} - \\mu)^2}{2\\sigma^2} \n",
        "%    \\\\\n",
        "%    &=  -\\frac{N}{2} \\log{\\left( 2\\pi\\sigma^2 \\right)}\n",
        "%    - \\frac{1}{2\\sigma^2} \\sum_{n=0}^{N-1}\\left(x_{n} - \\mu\\right)^2\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G6WvLLpj7Rvb"
      },
      "outputs": [],
      "source": [
        "# Our initial guesses..\n",
        "mu_initial_guess = 1.0\n",
        "sigma_initial_guess = np.sqrt(1.0)\n",
        "\n",
        "# The data to fit to (NOTE: this is our constant data so no gradients required)\n",
        "t_x_n = torch.tensor(x_n)\n",
        "\n",
        "# Note: mu and sigma are now *variables* not constants!\n",
        "# We need to specify their data type and initial value.. \n",
        "t_mu = torch.tensor(mu_initial_guess, requires_grad=True)\n",
        "t_sigma = torch.tensor(sigma_initial_guess, requires_grad=True)\n",
        "\n",
        "# Note: this step is important - don't use t_sigma directly!! \n",
        "t_sigma_2 = t_sigma ** 2.0\n",
        "\n",
        "# Calculate log p(X) terms..\n",
        "\n",
        "t_x_minus_mu_2 = (t_x_n - t_mu) ** 2.0\n",
        "t_denom = 2.0 * t_sigma_2\n",
        "t_sigma_term = - 0.5 * torch.log(2.0 * np.pi * t_sigma_2)\n",
        "\n",
        "t_log_P_terms = t_sigma_term - (t_x_minus_mu_2 / t_denom)\n",
        "\n",
        "# The sum is performed by a reduction in torch \n",
        "# (since a vector goes in and a scalar comes out)\n",
        "# but this is effectively the same as np.sum(...)\n",
        "t_log_P = torch.sum(t_log_P_terms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0Q8s9IZc7Rvd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8082140d-45d5-4f4f-b464-03d007af0b50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch log p(X) =  tensor(-56.5345, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
            "(using initial guesses for mu and sigma)\n",
            "\n",
            "Value from scipy stats package =  -56.53453487954605\n",
            "\n",
            "Everything working!\n"
          ]
        }
      ],
      "source": [
        "# Let's just check that we calculated things correctly:\n",
        "\n",
        "print('Torch log p(X) = ', t_log_P)\n",
        "print('(using initial guesses for mu and sigma)\\n')\n",
        "\n",
        "# Check with scipy..\n",
        "from scipy.stats import norm\n",
        "check_value = np.sum(norm.logpdf(x_n, \n",
        "                                 mu_initial_guess, \n",
        "                                 sigma_initial_guess))\n",
        "print('Value from scipy stats package = ', check_value)\n",
        "\n",
        "# Check these are close (to nummerical precision - remember not to use\n",
        "# equality when checking floating point numbers due to round-off error)..\n",
        "assert(np.isclose(t_log_P.detach().numpy(), check_value))\n",
        "\n",
        "print('\\nEverything working!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** torch.tensor values are not the same as numpy.arrays and so when passing torch values into numpy functions (as illustrated in the np.isclose call above). If we have a value that is current associated with a computational graph to calculate the gradient then we must also create a copy first by detaching it from the graph, hence the use of \n",
        "`t_log_P.detach().numpy()`\n"
      ],
      "metadata": {
        "id": "nCk8x3aSPXZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is great, but what if we want to use different parameter values?\n",
        "\n",
        "Well, we need to put our torch code into a function so that we can call it with different values. Let's put the code from above into a function:"
      ],
      "metadata": {
        "id": "kQEWcNdgQNMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the log likelihood function taking the data and parameters as arguments\n",
        "def torch_gaussian_log_likelihood(t_x_n, t_mu, t_sigma):\n",
        "    # Note: this step is important - don't use t_sigma directly!! \n",
        "    t_sigma_2 = t_sigma ** 2.0\n",
        "\n",
        "    # Calculate log p(X) terms..\n",
        "\n",
        "    t_x_minus_mu_2 = (t_x_n - t_mu) ** 2.0\n",
        "    t_denom = 2.0 * t_sigma_2\n",
        "    t_sigma_term = - 0.5 * torch.log(2.0 * np.pi * t_sigma_2)\n",
        "\n",
        "    t_log_P_terms = t_sigma_term - (t_x_minus_mu_2 / t_denom)\n",
        "\n",
        "    # The sum is performed by a reduction in torch \n",
        "    # (since a vector goes in and a scalar comes out)\n",
        "    # but this is effectively the same as np.sum(...)\n",
        "    t_log_P = torch.sum(t_log_P_terms)\n",
        "\n",
        "    return t_log_P\n",
        "\n",
        "\n",
        "# Let's check again..\n",
        "print('Torch log p(X) = ', torch_gaussian_log_likelihood(t_x_n, t_mu, t_sigma))\n",
        "print('(using initial guesses for mu and sigma)\\n')\n",
        "\n",
        "print('Value from scipy stats package = ', check_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGLfC6bYQmal",
        "outputId": "996d5fde-8700-444a-ab4a-ccbcd976c564"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch log p(X) =  tensor(-56.5345, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
            "(using initial guesses for mu and sigma)\n",
            "\n",
            "Value from scipy stats package =  -56.53453487954605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-cND5yl7Rvg"
      },
      "source": [
        "### So now let's use the power of pytorch!!\n",
        "\n",
        "To perform optimisation we need to know the gradient of the log likelihood with respect to the particular parameters $\\mu$ and $\\sigma$. \n",
        "\n",
        "We know how to find these with torch using the backward from above!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mRK7RHka7Rvg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fbd5b1a-f7e9-449d-c144-44b858db32d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient wrt mu =  tensor(26.1300)\n",
            "Gradient wrt sigma =  tensor(56.3115, dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "t_log_P = torch_gaussian_log_likelihood(t_x_n, t_mu, t_sigma)\n",
        "\n",
        "t_log_P.backward()\n",
        "\n",
        "print('Gradient wrt mu = ', t_mu.grad)\n",
        "print('Gradient wrt sigma = ', t_sigma.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqOC4ujq7Rvh"
      },
      "source": [
        "Shall we check that result. Remember we have:\n",
        "\n",
        "\\begin{align}\n",
        "\\log  p(X)  &= \\sum_{n=0}^{N-1} \n",
        "    -\\frac{1}{2} \\log{\\left( 2\\pi\\sigma^2 \\right)}\n",
        "    -\\frac{(x_{n} - \\mu)^2}{2\\sigma^2} \\\\\n",
        "    &=  -\\frac{N}{2} \\log{\\left( 2\\pi\\sigma^2 \\right)}\n",
        "    - \\frac{1}{2\\sigma^2} \\sum_{n=0}^{N-1}\\left(x_{n} - \\mu\\right)^2\n",
        "\\end{align}\n",
        "\n",
        "So for $\\mu$ we have:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial \\log  p(X)}{\\partial \\mu}   \n",
        "    &= - 0\n",
        "    - \\frac{1}{2\\sigma^2} \\frac{\\partial}{\\partial \\mu}  \\sum_{n=0}^{N-1} \\left(x_{n} - \\mu\\right)^2 \\\\\n",
        "    &= - \\frac{1}{2\\sigma^2} \\sum_{n=0}^{N-1} \\frac{\\partial}{\\partial \\mu} \\left(x_{n} - \\mu\\right)^2 \\\\\n",
        "    &= - \\frac{1}{2\\sigma^2} \\sum_{n=0}^{N-1} 2 \\left(x_{n} - \\mu\\right) \\frac{\\partial}{\\partial \\mu}\\left(x_{n} - \\mu\\right)  \\\\\n",
        "    &= \\frac{1}{\\sigma^2} \\sum_{n=0}^{N-1} \\left(x_{n} - \\mu\\right)\n",
        "\\end{align}\n",
        "\n",
        "where we used the chain rule a number of times.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5fDDV9cD7Rvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a875d63-08bc-45b3-b661-d48ca8f4fbb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our analytic gradient wrt mu =  26.130047695858497\n",
            "Torch gradient wrt mu =  tensor(26.1300)\n",
            "\n",
            "Excellent! torch calculated the gradient for us :)\n"
          ]
        }
      ],
      "source": [
        "# numpy check of gradient wrt mu\n",
        "\n",
        "grad_mu_check = np.sum(x_n - mu_initial_guess) / \\\n",
        "                (sigma_initial_guess ** 2)\n",
        "    \n",
        "print('Our analytic gradient wrt mu = ', grad_mu_check)\n",
        "\n",
        "print('Torch gradient wrt mu = ', t_mu.grad)\n",
        "\n",
        "assert(np.isclose(t_mu.grad.detach().numpy(), grad_mu_check))\n",
        "\n",
        "print('\\nExcellent! torch calculated the gradient for us :)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW9LE3_W7Rvj"
      },
      "source": [
        "## Everyone should now be in awe!\n",
        "\n",
        "This might seem like something trivial but hopefully you can see that actually quite a lot of maths and then coding went into determining the gradient. \n",
        "\n",
        "In fact, you can do the same to check the value for the gradient wrt $\\sigma^2$.\n",
        "\n",
        "When we calculated the result using the chain rule. Since torch built up a graph of the operations, it is able to apply the chain rule results for us automatically.\n",
        "\n",
        "This:\n",
        "\\begin{align}\n",
        "\\log  p(X)  &= \\sum_{n=0}^{N-1} \n",
        "    -\\frac{1}{2} \\log{\\left( 2\\pi\\sigma^2 \\right)}\n",
        "    -\\frac{(x_{n} - \\mu)^2}{2\\sigma^2} \n",
        "%    \\\\\n",
        "%    &=  -\\frac{N}{2} \\log{\\left( 2\\pi\\sigma^2 \\right)}\n",
        "%    - \\frac{1}{2\\sigma^2} \\sum_{n=0}^{N-1}\\left(x_{n} - \\mu\\right)^2\n",
        "\\end{align}\n",
        "has become a computational graph.\n",
        "\n",
        "For example, the `pow` operation represents $r = a^b$ for the inputs $a,b$ and result $r$. As above, torch then knows that $\\frac{\\partial r}{\\partial a} = b a^{b-1}$, and by chaining these operations together it can work backwards through the graph (from $\\log p(X)$ at the top to $\\mu$ at the bottom) to calculate the gradient.\n",
        "\n",
        "Therefore, the procedure when operating with torch is always the same. A **forward pass** can calculate the objective for the current set of parameters and then a **backwards pass** can calculate the gradients of an objective wrt any of the parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple gradient descent using these gradients..\n",
        "\n",
        "Let's use simple gradient descent to try to fit the values of our parameters to our data. Essentially, we start with some initial values and take a \"step\" in the direction of the (downhill) gradient in order to minimise the objective.\n",
        "\n",
        "In our case, the parameters that we want will maximise the log likelihood so the objective function we must use is to minimise the **negative** log likelihood.\n",
        "\n",
        "So the update rule will be:\n",
        "$$\n",
        "\\mu^{(k+1)} = \\mu^{(k)} - \\eta \\frac{\\partial}{\\partial \\mu} \\log p(\\mathbf{x})\n",
        "$$\n",
        "where $\\eta$ is some (small) step size and $k$ is the iteration index.\n",
        "\n",
        "Let us try to do this loop a few times:"
      ],
      "metadata": {
        "id": "nU0mHhKOTt8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_iterations = 100\n",
        "step_size_eta = 1.0e-2\n",
        "\n",
        "# The data to fit to (NOTE: this is our constant data so no gradients required)\n",
        "t_x_n = torch.tensor(x_n)\n",
        "\n",
        "# Note: mu and sigma are now *variables* not constants!\n",
        "# We need to specify their data type and initial value.. \n",
        "t_mu = torch.tensor(mu_initial_guess, requires_grad=True)\n",
        "t_sigma = torch.tensor(sigma_initial_guess, requires_grad=True)\n",
        "\n",
        "for iteration in range(number_of_iterations + 1):\n",
        "    # Perform the forward pass, calculate - log p(x) \n",
        "    t_neg_log_likelihood = -1.0 * torch_gaussian_log_likelihood(t_x_n, t_mu, t_sigma)\n",
        "\n",
        "    # Perform the backwards pass to get the gradients\n",
        "    t_neg_log_likelihood.backward()\n",
        "\n",
        "    # Temporarily disable gradient computations so that we can update the \n",
        "    # parameter values (we don't want to differentiate the update!)..\n",
        "    with torch.no_grad():\n",
        "        # Update the parameters based on the step size..\n",
        "        t_mu -= step_size_eta * t_mu.grad\n",
        "        t_sigma -= step_size_eta * t_sigma.grad\n",
        "\n",
        "        # IMPORTANT: Clear the gradients for next time..\n",
        "        t_mu.grad.data.zero_()\n",
        "        t_sigma.grad.data.zero_()\n",
        "\n",
        "    # Print out the current values\n",
        "    if iteration % 10 == 0:\n",
        "        print('Iter %04d, NLL %0.2e, mu %.4f, sigma %.4f' % \n",
        "              (iteration, t_neg_log_likelihood.detach().numpy(), t_mu, t_sigma))\n",
        "\n",
        "\n",
        "print('\\nAfter optimisation:')\n",
        "print('Torch mu = ', t_mu)\n",
        "print('Torch sigma = ', t_sigma)\n",
        "\n",
        "print('\\nAnalytic estimates:')\n",
        "print('Estimated mu = ', np.mean(x_n))\n",
        "print('Estimated std = ', np.std(x_n))\n",
        "\n",
        "print('\\nGround truth values:')\n",
        "print('True mu = ', mu_true)\n",
        "print('True sigma = ', sigma_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyITZL-ZTuVw",
        "outputId": "95995aec-0f79-4beb-f6c2-1875fd2f70db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 0000, NLL 5.65e+01, mu 1.2613, sigma 1.5631\n",
            "Iter 0010, NLL 3.71e+01, mu 1.8332, sigma 1.6005\n",
            "Iter 0020, NLL 3.61e+01, mu 2.1077, sigma 1.5026\n",
            "Iter 0030, NLL 3.59e+01, mu 2.2301, sigma 1.4632\n",
            "Iter 0040, NLL 3.58e+01, mu 2.2781, sigma 1.4541\n",
            "Iter 0050, NLL 3.58e+01, mu 2.2960, sigma 1.4524\n",
            "Iter 0060, NLL 3.58e+01, mu 2.3026, sigma 1.4522\n",
            "Iter 0070, NLL 3.58e+01, mu 2.3051, sigma 1.4521\n",
            "Iter 0080, NLL 3.58e+01, mu 2.3060, sigma 1.4521\n",
            "Iter 0090, NLL 3.58e+01, mu 2.3063, sigma 1.4521\n",
            "Iter 0100, NLL 3.58e+01, mu 2.3064, sigma 1.4521\n",
            "\n",
            "After optimisation:\n",
            "Torch mu =  tensor(2.3064, requires_grad=True)\n",
            "Torch sigma =  tensor(1.4521, dtype=torch.float64, requires_grad=True)\n",
            "\n",
            "Analytic estimates:\n",
            "Estimated mu =  2.3065023847929247\n",
            "Estimated std =  1.4521115453282711\n",
            "\n",
            "Ground truth values:\n",
            "True mu =  2.5\n",
            "True sigma =  1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Great - we can run an optimisation that automatically calculates the gradients!\n",
        "\n",
        "So we now have everything coming together to perform a nummerical optimisation where we can perform gradient descent without having to write any code to calculate the gradients ourselves (yay!!).\n",
        "\n",
        "The final stage is that there were a few technical gotchas in there around dealing with the parameter gradients and updating them (i.e. we had to temporaily suspend gradient calculation and ensure that we zeroed the gradient datastructures for each parameter.\n",
        "\n",
        "Torch provides a nicer way to modularise this process into an object (or class) in python that provides a nicer interfact. Each module has a set of parameters to be optimised and a forward operation to be performed on data. The autograd operations will then take care of performing the backward operation and an optimiser can be called to control how the parameters are updated (e.g. something more complicated than simple gradient descent).\n",
        "\n",
        "Let's wrap up our code in this interface.\n"
      ],
      "metadata": {
        "id": "r_J7ELAtaFh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting models into modules..\n",
        "\n",
        "Torch has a paradigm to make this procedure easy to work with based on deriving your own class based on a `module` that keeps parameters and the forward pass of the model together to allow easy optimisation."
      ],
      "metadata": {
        "id": "5dS4vPaJTYcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our own class derived from the torch module..\n",
        "class MaximumLikelihoodGaussianModel(torch.nn.Module):\n",
        "    # We must initialise our model - we specify our initial guesses for the \n",
        "    # parameters..\n",
        "    def __init__(self, mu_initial_guess, sigma_initial_guess):\n",
        "        # Call the constructor for the torch.nn.Module super class..\n",
        "        super().__init__()\n",
        "\n",
        "        # We use the Parameter class (rather than tensors) for the module\n",
        "        # but these behave in the same way..\n",
        "        self.t_mu = torch.nn.Parameter(torch.tensor(mu_initial_guess))\n",
        "        self.t_sigma = torch.nn.Parameter(torch.tensor(sigma_initial_guess))\n",
        "\n",
        "    # This defines the forward operation on some data passed in \n",
        "    def forward(self, t_x_n):\n",
        "\n",
        "        # We will copy in the code from torch_gaussian_log_likelihood..\n",
        "\n",
        "        # Note: this step is important - don't use t_sigma directly!! \n",
        "        t_sigma_2 = self.t_sigma ** 2.0\n",
        "\n",
        "        # Calculate log p(X) terms..\n",
        "\n",
        "        t_x_minus_mu_2 = (t_x_n - self.t_mu) ** 2.0\n",
        "        t_denom = 2.0 * t_sigma_2\n",
        "        t_sigma_term = - 0.5 * torch.log(2.0 * np.pi * t_sigma_2)\n",
        "\n",
        "        t_log_P_terms = t_sigma_term - (t_x_minus_mu_2 / t_denom)\n",
        "\n",
        "        # The sum is performed by a reduction in torch \n",
        "        # (since a vector goes in and a scalar comes out)\n",
        "        # but this is effectively the same as np.sum(...)\n",
        "        t_log_P = torch.sum(t_log_P_terms)\n",
        "\n",
        "        # Remember to take the negative to return the negative log likelihood..\n",
        "        return -1.0 * t_log_P        "
      ],
      "metadata": {
        "id": "hoaYa2SnbXvL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we can optimise our custom module\n",
        "\n",
        "We can now use the standard optimisation approach for torch using our custom module. The steps are:\n",
        "- Define an instance of our module\n",
        "- Define an optimiser\n",
        "- Loop for each iteration:\n",
        "    - Zero the gradients\n",
        "    - Perform the forward pass\n",
        "    - Perform the backward pass\n",
        "    - Take an optimiser step"
      ],
      "metadata": {
        "id": "k4M2qt4DdI-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_iterations = 100\n",
        "# Note the step size is also referred to as the learning rate..\n",
        "learning_rate = 1.0e-2\n",
        "\n",
        "# The data to fit to (NOTE: this is our constant data so no gradients required)\n",
        "t_x_n = torch.tensor(x_n)\n",
        "\n",
        "# Create our model - it will initialise the paramters appropriately\n",
        "gaussian_model = MaximumLikelihoodGaussianModel(mu_initial_guess, sigma_initial_guess)\n",
        "gaussian_model.train()\n",
        "\n",
        "# Create an optimiser for our model, the model has a datastructure of all\n",
        "# the parameters to optimise that we will pass in (along with the learning rate)\n",
        "optimizer = torch.optim.SGD(gaussian_model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iteration in range(number_of_iterations + 1):\n",
        "    # Perform the forward pass by calling the model with the data..\n",
        "    t_neg_log_likelihood = gaussian_model(t_x_n)\n",
        "\n",
        "    # Clear the gradients..\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform the backwards pass to calculate the gradients..\n",
        "    t_neg_log_likelihood.backward()\n",
        "\n",
        "    # Update the parameters via the optimiser..\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print out the current values\n",
        "    if iteration % 10 == 0:\n",
        "        print('Iter %04d, NLL %0.2e, mu %.4f, sigma %.4f' % \n",
        "              (iteration, t_neg_log_likelihood.item(), \n",
        "               gaussian_model.t_mu, gaussian_model.t_sigma))\n",
        "\n",
        "\n",
        "print('\\nAfter optimisation:')\n",
        "print('Torch mu = ', gaussian_model.t_mu.item())\n",
        "print('Torch sigma = ', gaussian_model.t_sigma.item())\n",
        "\n",
        "print('\\nAnalytic estimates:')\n",
        "print('Estimated mu = ', np.mean(x_n))\n",
        "print('Estimated std = ', np.std(x_n))\n",
        "\n",
        "print('\\nGround truth values:')\n",
        "print('True mu = ', mu_true)\n",
        "print('True sigma = ', sigma_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15AFQLIbdJT6",
        "outputId": "6e9c110e-53e6-4dab-dd9b-b0a5d7157e8d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 0000, NLL 5.65e+01, mu 1.2613, sigma 1.5631\n",
            "Iter 0010, NLL 3.71e+01, mu 1.8332, sigma 1.6005\n",
            "Iter 0020, NLL 3.61e+01, mu 2.1077, sigma 1.5026\n",
            "Iter 0030, NLL 3.59e+01, mu 2.2301, sigma 1.4632\n",
            "Iter 0040, NLL 3.58e+01, mu 2.2781, sigma 1.4541\n",
            "Iter 0050, NLL 3.58e+01, mu 2.2960, sigma 1.4524\n",
            "Iter 0060, NLL 3.58e+01, mu 2.3026, sigma 1.4522\n",
            "Iter 0070, NLL 3.58e+01, mu 2.3051, sigma 1.4521\n",
            "Iter 0080, NLL 3.58e+01, mu 2.3060, sigma 1.4521\n",
            "Iter 0090, NLL 3.58e+01, mu 2.3063, sigma 1.4521\n",
            "Iter 0100, NLL 3.58e+01, mu 2.3064, sigma 1.4521\n",
            "\n",
            "After optimisation:\n",
            "Torch mu =  2.3064305782318115\n",
            "Torch sigma =  1.4521115701934892\n",
            "\n",
            "Analytic estimates:\n",
            "Estimated mu =  2.3065023847929247\n",
            "Estimated std =  1.4521115453282711\n",
            "\n",
            "Ground truth values:\n",
            "True mu =  2.5\n",
            "True sigma =  1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Great - let's visualise our working model..\n",
        "\n",
        "Our new torch module allows us to perform the optimisation of our parameters using a standard approach (we can change the model and the rest of the code is unaltered). \n",
        "\n",
        "**We can also visualise the computational graph created by our model with the follow code:**"
      ],
      "metadata": {
        "id": "nxF7W1PWm-e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "make_dot(gaussian_model(t_x_n), params=dict(gaussian_model.named_parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "AN8FbjhTm_M8",
        "outputId": "9cdd1b6e-0307-41ba-decd-aba0c883269b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fa40e596790>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"317pt\" height=\"545pt\"\n viewBox=\"0.00 0.00 317.00 545.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 541)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-541 313,-541 313,4 -4,4\"/>\n<!-- 140342592134416 -->\n<g id=\"node1\" class=\"node\">\n<title>140342592134416</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"237.5,-31 183.5,-31 183.5,0 237.5,0 237.5,-31\"/>\n<text text-anchor=\"middle\" x=\"210.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 140342592103056 -->\n<g id=\"node2\" class=\"node\">\n<title>140342592103056</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"255,-86 166,-86 166,-67 255,-67 255,-86\"/>\n<text text-anchor=\"middle\" x=\"210.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140342592103056&#45;&gt;140342592134416 -->\n<g id=\"edge16\" class=\"edge\">\n<title>140342592103056&#45;&gt;140342592134416</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M210.5,-66.9688C210.5,-60.1289 210.5,-50.5621 210.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"214.0001,-41.3678 210.5,-31.3678 207.0001,-41.3678 214.0001,-41.3678\"/>\n</g>\n<!-- 140342592102800 -->\n<g id=\"node3\" class=\"node\">\n<title>140342592102800</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"255,-141 166,-141 166,-122 255,-122 255,-141\"/>\n<text text-anchor=\"middle\" x=\"210.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SumBackward0</text>\n</g>\n<!-- 140342592102800&#45;&gt;140342592103056 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140342592102800&#45;&gt;140342592103056</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M210.5,-121.9197C210.5,-114.9083 210.5,-105.1442 210.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"214.0001,-96.3408 210.5,-86.3408 207.0001,-96.3409 214.0001,-96.3408\"/>\n</g>\n<!-- 140342592102864 -->\n<g id=\"node4\" class=\"node\">\n<title>140342592102864</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"255,-196 166,-196 166,-177 255,-177 255,-196\"/>\n<text text-anchor=\"middle\" x=\"210.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SubBackward0</text>\n</g>\n<!-- 140342592102864&#45;&gt;140342592102800 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140342592102864&#45;&gt;140342592102800</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M210.5,-176.9197C210.5,-169.9083 210.5,-160.1442 210.5,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"214.0001,-151.3408 210.5,-141.3408 207.0001,-151.3409 214.0001,-151.3408\"/>\n</g>\n<!-- 140342592102928 -->\n<g id=\"node5\" class=\"node\">\n<title>140342592102928</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"300,-251 211,-251 211,-232 300,-232 300,-251\"/>\n<text text-anchor=\"middle\" x=\"255.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140342592102928&#45;&gt;140342592102864 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140342592102928&#45;&gt;140342592102864</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M247.6616,-231.9197C241.4151,-224.2851 232.4982,-213.3867 224.9648,-204.1792\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"227.5929,-201.8641 218.5516,-196.3408 222.1751,-206.2968 227.5929,-201.8641\"/>\n</g>\n<!-- 140342592102608 -->\n<g id=\"node6\" class=\"node\">\n<title>140342592102608</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"309,-306 220,-306 220,-287 309,-287 309,-306\"/>\n<text text-anchor=\"middle\" x=\"264.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">LogBackward0</text>\n</g>\n<!-- 140342592102608&#45;&gt;140342592102928 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140342592102608&#45;&gt;140342592102928</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M262.9323,-286.9197C261.785,-279.9083 260.1872,-270.1442 258.767,-261.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"262.1793,-260.6443 257.1103,-251.3408 255.2712,-261.7748 262.1793,-260.6443\"/>\n</g>\n<!-- 140342592103568 -->\n<g id=\"node7\" class=\"node\">\n<title>140342592103568</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"309,-361 220,-361 220,-342 309,-342 309,-361\"/>\n<text text-anchor=\"middle\" x=\"264.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140342592103568&#45;&gt;140342592102608 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140342592103568&#45;&gt;140342592102608</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M264.5,-341.9197C264.5,-334.9083 264.5,-325.1442 264.5,-316.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"268.0001,-316.3408 264.5,-306.3408 261.0001,-316.3409 268.0001,-316.3408\"/>\n</g>\n<!-- 140342592103440 -->\n<g id=\"node8\" class=\"node\">\n<title>140342592103440</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"261,-416 172,-416 172,-397 261,-397 261,-416\"/>\n<text text-anchor=\"middle\" x=\"216.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">PowBackward0</text>\n</g>\n<!-- 140342592103440&#45;&gt;140342592103568 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140342592103440&#45;&gt;140342592103568</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M224.861,-396.9197C231.5919,-389.2072 241.2297,-378.1639 249.3164,-368.8978\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"251.9733,-371.1765 255.9116,-361.3408 246.6993,-366.5737 251.9733,-371.1765\"/>\n</g>\n<!-- 140346158898448 -->\n<g id=\"node16\" class=\"node\">\n<title>140346158898448</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"202,-361 113,-361 113,-342 202,-342 202,-361\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140342592103440&#45;&gt;140346158898448 -->\n<g id=\"edge15\" class=\"edge\">\n<title>140342592103440&#45;&gt;140346158898448</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M206.223,-396.9197C197.7824,-389.0514 185.623,-377.7164 175.5631,-368.3385\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"177.7578,-365.5995 168.0565,-361.3408 172.9846,-370.7198 177.7578,-365.5995\"/>\n</g>\n<!-- 140346158896912 -->\n<g id=\"node9\" class=\"node\">\n<title>140346158896912</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"267,-471 166,-471 166,-452 267,-452 267,-471\"/>\n<text text-anchor=\"middle\" x=\"216.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140346158896912&#45;&gt;140342592103440 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140346158896912&#45;&gt;140342592103440</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M216.5,-451.9197C216.5,-444.9083 216.5,-435.1442 216.5,-426.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"220.0001,-426.3408 216.5,-416.3408 213.0001,-426.3409 220.0001,-426.3408\"/>\n</g>\n<!-- 140342592065456 -->\n<g id=\"node10\" class=\"node\">\n<title>140342592065456</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"246,-537 187,-537 187,-507 246,-507 246,-537\"/>\n<text text-anchor=\"middle\" x=\"216.5\" y=\"-525\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">t_sigma</text>\n<text text-anchor=\"middle\" x=\"216.5\" y=\"-514\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 140342592065456&#45;&gt;140346158896912 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140342592065456&#45;&gt;140346158896912</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M216.5,-506.7333C216.5,-499.0322 216.5,-489.5977 216.5,-481.3414\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"220.0001,-481.0864 216.5,-471.0864 213.0001,-481.0864 220.0001,-481.0864\"/>\n</g>\n<!-- 140342592106128 -->\n<g id=\"node11\" class=\"node\">\n<title>140342592106128</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"202,-306 113,-306 113,-287 202,-287 202,-306\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">DivBackward0</text>\n</g>\n<!-- 140342592106128&#45;&gt;140342592102864 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140342592106128&#45;&gt;140342592102864</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M162.2034,-286.7382C170.9866,-268.5089 190.0198,-229.0062 201.3943,-205.3986\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"204.6952,-206.6109 205.8828,-196.0828 198.3891,-203.5724 204.6952,-206.6109\"/>\n</g>\n<!-- 140346158898832 -->\n<g id=\"node12\" class=\"node\">\n<title>140346158898832</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"95,-361 6,-361 6,-342 95,-342 95,-361\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">PowBackward0</text>\n</g>\n<!-- 140346158898832&#45;&gt;140342592106128 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140346158898832&#45;&gt;140342592106128</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M69.138,-341.9197C85.8362,-333.3365 110.5598,-320.6281 129.6118,-310.8351\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"131.4228,-313.8395 138.7166,-306.155 128.2227,-307.6138 131.4228,-313.8395\"/>\n</g>\n<!-- 140342592105872 -->\n<g id=\"node13\" class=\"node\">\n<title>140342592105872</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"95,-416 6,-416 6,-397 95,-397 95,-416\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SubBackward0</text>\n</g>\n<!-- 140342592105872&#45;&gt;140346158898832 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140342592105872&#45;&gt;140346158898832</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-396.9197C50.5,-389.9083 50.5,-380.1442 50.5,-371.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-371.3408 50.5,-361.3408 47.0001,-371.3409 54.0001,-371.3408\"/>\n</g>\n<!-- 140342592106384 -->\n<g id=\"node14\" class=\"node\">\n<title>140342592106384</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-471 0,-471 0,-452 101,-452 101,-471\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140342592106384&#45;&gt;140342592105872 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140342592106384&#45;&gt;140342592105872</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-451.9197C50.5,-444.9083 50.5,-435.1442 50.5,-426.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-426.3408 50.5,-416.3408 47.0001,-426.3409 54.0001,-426.3408\"/>\n</g>\n<!-- 140342592063248 -->\n<g id=\"node15\" class=\"node\">\n<title>140342592063248</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"77.5,-537 23.5,-537 23.5,-507 77.5,-507 77.5,-537\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-525\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">t_mu</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-514\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 140342592063248&#45;&gt;140342592106384 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140342592063248&#45;&gt;140342592106384</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-506.7333C50.5,-499.0322 50.5,-489.5977 50.5,-481.3414\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-481.0864 50.5,-471.0864 47.0001,-481.0864 54.0001,-481.0864\"/>\n</g>\n<!-- 140346158898448&#45;&gt;140342592106128 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140346158898448&#45;&gt;140342592106128</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M157.5,-341.9197C157.5,-334.9083 157.5,-325.1442 157.5,-316.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"161.0001,-316.3408 157.5,-306.3408 154.0001,-316.3409 161.0001,-316.3408\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows us all the functions we would have to have implemented in order to calculate the gradients in the backward pass - we got all these for free using torch instead of numpy!!"
      ],
      "metadata": {
        "id": "sT4hecTCncE5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN39WiDJ7Rvj"
      },
      "source": [
        "## But the fun doesn't end here!\n",
        "\n",
        "We can now run the same code again with different data just by changing the data we pass in to the model. This is how minibatching can be performed where we use a different subset of the data at each iteration (if we have very large datasets) to estimate the gradients to perform **stochastic gradient descent**.\n",
        "\n",
        "Let's try with a larger dataset..\n",
        "\n",
        "#### More information on dataset loading:\n",
        "\n",
        "[Datasets and Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_bigger = 1000\n",
        "\n",
        "number_of_iterations = 100\n",
        "# Note the step size is also referred to as the learning rate..\n",
        "learning_rate = 1.0e-3\n",
        "\n",
        "x_bigger = np.random.normal(mu_true, sigma_true, N_bigger)\n",
        "t_x_n = torch.tensor(x_bigger)\n",
        "\n",
        "# Create our model - it will initialise the paramters appropriately\n",
        "gaussian_model = MaximumLikelihoodGaussianModel(mu_initial_guess, sigma_initial_guess)\n",
        "gaussian_model.train()\n",
        "\n",
        "# Create an optimiser for our model, the model has a datastructure of all\n",
        "# the parameters to optimise that we will pass in (along with the learning rate)\n",
        "optimizer = torch.optim.SGD(gaussian_model.parameters(), lr=learning_rate)\n",
        "\n",
        "print([a for a in gaussian_model.parameters()])\n",
        "\n",
        "for iteration in range(number_of_iterations + 1):\n",
        "    # Perform the forward pass by calling the model with the data..\n",
        "    t_neg_log_likelihood = gaussian_model(t_x_n)\n",
        "\n",
        "    # Clear the gradients..\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform the backwards pass to calculate the gradients..\n",
        "    t_neg_log_likelihood.backward()\n",
        "\n",
        "    # Update the parameters via the optimiser..\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print out the current values\n",
        "    if iteration % 10 == 0:\n",
        "        print('Iter %04d, NLL %0.2e, mu %.4f, sigma %.4f' % \n",
        "              (iteration, t_neg_log_likelihood.item(), \n",
        "               gaussian_model.t_mu, gaussian_model.t_sigma))\n",
        "\n",
        "\n",
        "print('\\nAfter optimisation:')\n",
        "print('Torch mu = ', gaussian_model.t_mu.item())\n",
        "print('Torch sigma = ', gaussian_model.t_sigma.item())\n",
        "\n",
        "print('\\nAnalytic estimates:')\n",
        "print('Estimated mu = ', np.mean(x_n))\n",
        "print('Estimated std = ', np.std(x_n))\n",
        "\n",
        "print('\\nGround truth values:')\n",
        "print('True mu = ', mu_true)\n",
        "print('True sigma = ', sigma_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3S3awmJhqQr",
        "outputId": "14e996d3-1627-48c7-db36-5e222eb29801"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor(1., requires_grad=True), Parameter containing:\n",
            "tensor(1., dtype=torch.float64, requires_grad=True)]\n",
            "Iter 0000, NLL 3.30e+03, mu 2.5681, sigma 4.7551\n",
            "Iter 0010, NLL 2.10e+03, mu 2.5681, sigma 2.5531\n",
            "Iter 0020, NLL 1.83e+03, mu 2.5681, sigma 1.5153\n",
            "Iter 0030, NLL 1.83e+03, mu 2.5681, sigma 1.5153\n",
            "Iter 0040, NLL 1.83e+03, mu 2.5681, sigma 1.5153\n",
            "Iter 0050, NLL 1.83e+03, mu 2.5681, sigma 1.5153\n",
            "Iter 0060, NLL 1.83e+03, mu 2.5681, sigma 1.5153\n",
            "Iter 0070, NLL 1.83e+03, mu 2.5681, sigma 1.5153\n",
            "Iter 0080, NLL 1.83e+03, mu 2.5681, sigma 1.5153\n",
            "Iter 0090, NLL 1.83e+03, mu 2.5681, sigma 1.5153\n",
            "Iter 0100, NLL 1.83e+03, mu 2.5681, sigma 1.5153\n",
            "\n",
            "After optimisation:\n",
            "Torch mu =  2.5680675506591797\n",
            "Torch sigma =  1.5153452365563655\n",
            "\n",
            "Analytic estimates:\n",
            "Estimated mu =  2.3065023847929247\n",
            "Estimated std =  1.4521115453282711\n",
            "\n",
            "Ground truth values:\n",
            "True mu =  2.5\n",
            "True sigma =  1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNcBcaXZ7Rvp"
      },
      "source": [
        "## All sorts of more advanced topics\n",
        "\n",
        "- Visualise parts of computation (e.g. Tensorboard)\n",
        "- Reusable components (e.g. modules for neural networks / classifiers / etc..)\n",
        "- Run computations on the GPU instead of the CPU (often faster)\n",
        "- Easy to scale; can distribute computations over an entire cluster!\n",
        "\n",
        "#### More references:\n",
        "\n",
        "[PyTorch Tutorials](https://pytorch.org/tutorials/index.html)\n",
        "\n",
        "[PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "abiEL3jB6G18"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f3in511lmt0G"
      },
      "execution_count": 18,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PyTorch Example.ipynb",
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}