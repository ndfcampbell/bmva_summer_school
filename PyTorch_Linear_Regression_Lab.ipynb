{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko5w7vwuBKKb"
      },
      "source": [
        "# Linear Regression in PyTorch\n",
        "\n",
        "This exercise takes you through the fundamental linear regression model from a number of different angles. First we consider an analytic analysis and then we continue to consider how to solve the same problem using numerical methods. This lab also serves and an introduction to the pytorch toolkit which will be useful for a variety of machine learning tasks in the future and is used by Google to solve massive machine learning problems on their clusters.\n",
        "\n",
        "The linear regression model forms the basis for a whole host of models - if you are comfortable with the fundamental approaches we take here, there will be a whole range of extensions, advances and applications available to you in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2mvebO3BKKc"
      },
      "source": [
        "For this lab exercise there are 6 places where you are expected to enter your own code. Every place you have to add code is indicated by\n",
        "\n",
        "`# Add your code here ..`\n",
        "\n",
        "`# ***********************************************************`\n",
        "\n",
        "with instructions above the code block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iUfHyO6QBKKd"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    in_colab = False\n",
        "    import google.colab\n",
        "    in_colab = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Use the following to access torch and tensorboard when running on colab\n",
        "if in_colab:\n",
        "    !pip install -U torch \n",
        "\n",
        "# New for today! Import PyTorch (refered to by package name torch)\n",
        "import torch\n",
        "\n",
        "from sklearn import datasets as ds\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the diabetes dataset\n",
        "x_raw, y_raw = ds.load_diabetes(return_X_y=True)\n",
        "\n",
        "# Use only one feature (S5 LTG)\n",
        "x_raw = x_raw[:, 8]"
      ],
      "metadata": {
        "id": "uj7k4rh5L-p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952VEwyQBKKe"
      },
      "source": [
        "## Diabetes dataset\n",
        "\n",
        "Ten baseline variables, age, sex, body mass index, average blood\n",
        "pressure, and six blood serum measurements were obtained for each of n =\n",
        "442 diabetes patients, as well as the response of interest, a\n",
        "quantitative measure of disease progression one year after baseline.\n",
        "\n",
        "**Data Set Characteristics:**\n",
        "\n",
        " - Number of Instances: 442\n",
        "\n",
        " - Number of Attributes: First 10 columns are numeric predictive values\n",
        "\n",
        " - Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
        "\n",
        " - Attribute Information:\n",
        "      - age     age in years\n",
        "      - sex\n",
        "      - bmi     body mass index\n",
        "      - bp      average blood pressure\n",
        "      - s1      tc, total serum cholesterol\n",
        "      - s2      ldl, low-density lipoproteins\n",
        "      - s3      hdl, high-density lipoproteins\n",
        "      - s4      tch, total cholesterol / HDL\n",
        "      - s5      ltg, possibly log of serum triglycerides level\n",
        "      - s6      glu, blood sugar level\n",
        "\n",
        "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
        "\n",
        "Source URL:\n",
        "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
        "\n",
        "For more information see:\n",
        "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
        "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i50Kih6DBKKe"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "We are going to look at the relationship between the \"average number of rooms per dwelling\" and median house price in the Boston dataset. First let us partition the data into a training and test split. We are going for 60% training and 40% testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csZh-q7SBKKf"
      },
      "outputs": [],
      "source": [
        "total_count = x_raw.shape[0]\n",
        "\n",
        "split = int(total_count * 0.6)\n",
        "\n",
        "# Shuffle the data to avoid any ordering bias..\n",
        "np.random.seed(0)\n",
        "shuffle = np.random.permutation(total_count)\n",
        "\n",
        "x = x_raw[shuffle]\n",
        "y = y_raw[shuffle]\n",
        "\n",
        "x_train_unnormalised = x[:split]\n",
        "y_train_unnormalised = y[:split]\n",
        "\n",
        "x_test_unnormalised = x[split:]\n",
        "y_test_unnormalised = y[split:]\n",
        "\n",
        "print('Training set size:', x_train_unnormalised.shape[0])\n",
        "print('Test set size:', x_test_unnormalised.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YPlaWXeBKKg"
      },
      "source": [
        "## Data Visualisation\n",
        "\n",
        "To allow for easy visualisation as you progress through the task we are using a single dimensional data set. Both the input $x$ and output $y$ are scalars so we can plot them on a standard scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKiR3_skBKKg"
      },
      "outputs": [],
      "source": [
        "# You can use this function to plot the data and then add your own plots on top..\n",
        "def plot_data(x, y):\n",
        "    plt.figure(figsize=[10,8])\n",
        "    plt.plot(x, y, 'b+')\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('Average number rooms per dwelling')\n",
        "    plt.ylabel('S5 serum triglycerides level')\n",
        "\n",
        "plot_data(x_train_unnormalised, y_train_unnormalised)\n",
        "plt.title('Plot of the Training Data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYk8pzd1BKKg"
      },
      "source": [
        "### Task 1:\n",
        "\n",
        "Write a function that normalises a vector of values. It should output a corresponding vector where the values have a mean of zero and a standard deviation of 1. Note that you should only perform an affine transformation of the data (i.e. a linear scaling and a fixed offset). This means that you must find $a$ and $b$ for $v_i = a u_i + b$ where $u$ is the input data and $v$ is the normalised output data.\n",
        "\n",
        "Your function should return the normalising constants as well as the normalised data.\n",
        "\n",
        "Write a second function that removes the normalisation and returns the data to its original values.\n",
        "\n",
        "Check that passing both `x_train` and `y_train` through both functions returns the vectors to their original values.\n",
        "\n",
        "*Hints:*\n",
        "- You might want to look at `np.all()` for the Boolean check that they return to their values.\n",
        "- When checking that floating point values are equal up to nummerical precision (e.g. rounding errors in the computations) you can use the `np.isclose()` function.\n",
        "- You can use the `assert()` command to guarantee that a statement is `True` before the program continues.\n",
        "\n",
        "*Points to consider:* \n",
        "- Why might it be sensible to normalise the data in the fashion described?\n",
        "- Considering that we are about to perform Linear Regression, why might we not want to perform a more involved normalisation process?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fDmHPCgBKKh"
      },
      "outputs": [],
      "source": [
        "def normalise_data(x_unnormalised):\n",
        "    # Add your code here..\n",
        "    # ************************************************************\n",
        "    # ...\n",
        "    \n",
        "    \n",
        "    \n",
        "    return x_normalised, a, b\n",
        "\n",
        "def unnormalise_data(x_normalised, a, b):\n",
        "    # Add your code here..\n",
        "    # ************************************************************\n",
        "    # ...\n",
        "    \n",
        "    \n",
        "    \n",
        "    return x_unnormalised\n",
        "\n",
        "x_train, x_norm_a, x_norm_b = normalise_data(x_train_unnormalised)\n",
        "y_train, y_norm_a, y_norm_b = normalise_data(y_train_unnormalised)\n",
        "\n",
        "x_test, _, _ = normalise_data(x_test_unnormalised)\n",
        "y_test, _, _ = normalise_data(y_test_unnormalised)\n",
        "\n",
        "# Add your code here to check that the unnormaliseding the \n",
        "# training data returns to their original values..\n",
        "# ************************************************************\n",
        "# ...\n",
        "\n",
        "\n",
        "\n",
        "# Plot the data to make sure they are normalised..\n",
        "plot_data(x_train, y_train)\n",
        "plt.title('Plot of the (Normalised) Training Data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6zBdzIiBKKh"
      },
      "source": [
        "## The Linear Regression Model\n",
        "\n",
        "In linear regression we are trying to fit a linear model to the data of the form\n",
        "\n",
        "\\begin{align}\n",
        "y &= w x + c\n",
        "\\end{align}\n",
        "\n",
        "where $w$ and $c$ are parameters to be learned that take the input data $x$ to the output data $y$. Once this model has been learned, we can use the parameters to predict the values of the output that would correspond to new values of the input.\n",
        "\n",
        "In order to determine the parameters, we need an objective function that we seek to optimise: this function returns a scalar value for all possible parameter values and we seek to change the parameters until the best scalar value is obtained.\n",
        "\n",
        "For linear regression, we usually take the objective as one which minimises the squared error; this is known as a linear least squares problem.\n",
        "\n",
        "*Aside: Think about what this means in terms of a model for the data when you have $y = f(x) + \\eta$ with $f(x)$ as a linear function $f(x) = w x + c$ and $\\eta$ as iid Gaussian noise.*\n",
        "\n",
        "Therefore our objective is given by the sum of squared differences between the true value of $y_i$ and the value estimated by our model $w x_i + c$.\n",
        "\n",
        "\\begin{align}\n",
        "E(w,c) &= \\sum_{i=0}^{N-1} \\big(y_i - f(x_i) \\big)^2 \\\\\n",
        "    &= \\sum_{i=0}^{N-1} \\big(y_i - (w x_i + c) \\big)^2 \\\\\n",
        "    &= \\sum_{i=0}^{N-1} \\big(y_i - w x_i - c \\big)^2\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q95a31hqBKKi"
      },
      "source": [
        "## Task 2\n",
        "\n",
        "Write a function that calculates the least squared error on the training data for a particular value of the parameters $w$ and $c$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIeynNrxBKKi"
      },
      "outputs": [],
      "source": [
        "def least_squares_error(x, y, w, c):\n",
        "    # Add code to calcuate the squared_error = E(w,c)..\n",
        "    # ************************************************************\n",
        "    # ...\n",
        "    \n",
        "    \n",
        "    \n",
        "    return squared_error\n",
        "\n",
        "print('Squared error for w = 1.5, c = 0.5 is ', \n",
        "      least_squares_error(x_train, y_train, w=1.5, c=0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzSkM97KBKKj"
      },
      "source": [
        "## Analytic Solution\n",
        "\n",
        "In the case of linear regression, we can find an analytic solution to this problem by finding stationary point of the objective function. We do this by evaluating the partial derivatives of the objective wrt each parameter in turn and setting them to zero. If we can then find a solution to these simultaneous equations, we have found an optimal setting for the parameters.\n",
        "\n",
        "For $w$ we have:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial E}{\\partial w} \n",
        "    &= \\frac{\\partial}{\\partial w}\\sum_{i=0}^{N-1} \\big(y_i - w x_i - c \\big)^2 \\\\\n",
        "    &= \\sum_{i=0}^{N-1} \\frac{\\partial}{\\partial w} \\big(y_i - w x_i - c \\big)^2 \\\\\n",
        "    &= \\sum_{i=0}^{N-1} 2 \\big(y_i - w x_i - c \\big) \\frac{\\partial}{\\partial w} \\big(y_i - w x_i - c \\big) \\\\\n",
        "    &= \\sum_{i=0}^{N-1} 2 \\big(y_i - w x_i - c \\big) \\big(- x_i \\big) \\\\\n",
        "    &= \\sum_{i=0}^{N-1} 2 x_i (w x_i + c - y_i) \\\\\n",
        "\\end{align}\n",
        "\n",
        "For $c$ we have:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial E}{\\partial c} \n",
        "    &= \\frac{\\partial}{\\partial c}\\sum_{i=0}^{N-1} \\big(y_i - w x_i - c \\big)^2 \\\\\n",
        "    &= \\sum_{i=0}^{N-1} \\frac{\\partial}{\\partial c} \\big(y_i - w x_i - c \\big)^2 \\\\\n",
        "    &= \\sum_{i=0}^{N-1} 2 \\big(y_i - w x_i - c \\big) \\frac{\\partial}{\\partial c} \\big(y_i - w x_i - c \\big) \\\\\n",
        "    &= \\sum_{i=0}^{N-1} 2 \\big(y_i - w x_i - c \\big) \\big(- 1 \\big) \\\\\n",
        "    &= \\sum_{i=0}^{N-1} 2 (w x_i + c - y_i) \\\\\n",
        "\\end{align}\n",
        "\n",
        "Now setting $\\frac{\\partial E}{\\partial w} = 0$:\n",
        "\n",
        "\\begin{align}\n",
        "\\Rightarrow \\sum_{i=0}^{N-1} x_i (w x_i + c - y_i) &= 0 \\\\\n",
        "w \\sum_{i=0}^{N-1} (x_i)^2 + c \\sum_{i=0}^{N-1} (x_i) - \\sum_{i=0}^{N-1} (x_i y_i) &= 0 \\\\\n",
        "\\Rightarrow A w + B c - C &= 0\n",
        "\\end{align}\n",
        "\n",
        "with $A = \\sum_{i=0}^{N-1} (x_i)^2$, $B = \\sum_{i=0}^{N-1} (x_i)$ and $C = \\sum_{i=0}^{N-1} (x_i y_i)$.\n",
        "\n",
        "Now setting $\\frac{\\partial E}{\\partial c} = 0$:\n",
        "\n",
        "\\begin{align}\n",
        "\\Rightarrow \\sum_{i=0}^{N-1} (w x_i + c - y_i) &= 0 \\\\\n",
        "w \\sum_{i=0}^{N-1} (x_i) + N c - \\sum_{i=0}^{N-1} (y_i) &= 0 \\\\\n",
        "\\Rightarrow B w + N c - D &= 0\n",
        "\\end{align}\n",
        "\n",
        "with $D = \\sum_{i=0}^{N-1} (y_i)$.\n",
        "\n",
        "Combining these two:\n",
        "\n",
        "\\begin{align}\n",
        "\\Rightarrow c = \\frac{1}{N} (D - B w) \\\\\n",
        "\\Rightarrow A w + \\frac{B}{N} (D - B w) - C = 0 \\\\\n",
        "\\Rightarrow w \\left(A - \\frac{B^2}{N}\\right) = \\left(C - \\frac{BD}{N}\\right) \\\\\n",
        "\\Rightarrow w = \\frac{CN - BD}{AN - B^2} \\\\\n",
        "\\Rightarrow c = \\frac{1}{N} \\left(D - B \\frac{\\left(CN - BD\\right)}{\\left(AN - B^2\\right)}\\right) \\\\\n",
        "\\Rightarrow c = \\frac{1}{N} \\left(\\frac{D \\left(AN - B^2\\right) - B C N + B^2 D}{\\left(AN - B^2\\right)}\\right) \\\\\n",
        "\\Rightarrow c = \\frac{AD - BC}{AN - B^2}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtAduu8KBKKj"
      },
      "source": [
        "## Task 3\n",
        "\n",
        "Write a function using numpy that takes as input `x_train` and `y_train` and outputs the optimal $w$ and $c$ parameters for least squares linear regression.\n",
        "\n",
        "Confirm that your results are reasonable by plotting the resulting linear function on the training graph as well as the predicted values for the test set on a separate graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZt8ile_BKKk"
      },
      "outputs": [],
      "source": [
        "def least_squares_analytic_solution(x, y):\n",
        "    # Add code to calcuate the optimal w and c ..\n",
        "    # ************************************************************\n",
        "    # ...\n",
        "    \n",
        "    \n",
        "    \n",
        "    return w, c\n",
        "\n",
        "w_opt, c_opt = least_squares_analytic_solution(x_train, y_train)\n",
        "\n",
        "print('Analytic solution:')\n",
        "print('Analytic w = ', w_opt)\n",
        "print('Analytic c = ', c_opt)\n",
        "\n",
        "plot_data(x_train, y_train)\n",
        "plt.title('Analytic Linear Regression (Training Data)')\n",
        "# Add code to plot a line showing your solution \n",
        "# for w and c..\n",
        "# ************************************************************\n",
        "# ...\n",
        "\n",
        "\n",
        "\n",
        "plot_data(x_test, y_test)\n",
        "plt.title('Analytic Linear Regression (Testing Data)')\n",
        "# Add code to plot your predictions for the x_test data\n",
        "# for comparison against y_test..\n",
        "# ************************************************************\n",
        "# ...\n",
        "\n",
        "\n",
        "\n",
        "print('Mean least squares error on TRAINING data = ',\n",
        "     least_squares_error(x_train, y_train, w_opt, c_opt) / x_train.shape[0])\n",
        "\n",
        "print('Mean least squares error on TEST data = ',\n",
        "     least_squares_error(x_test, y_test, w_opt, c_opt) / x_test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNfRwKV3BKKl"
      },
      "source": [
        "## PyTorch Optimisation\n",
        "\n",
        "**For help with this please refer to the separate pytorch example that was worked through in class and is available on moodle as a separate jupyter notebook. Please load the example notebook and run through it yourself again before continuing with this task.**\n",
        "\n",
        "In the previous task we looked at an analytic solution to the least square problem. We now pretend that we could not solve the problem analytically. Although this is not true for this case, the additional of extensions to the linear regression model (for example to improve robustness, adding feature selection or handling non-linear data) can mean that it is no longer possible to find an analytic solution and numerical optimisation must be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GuPjjgkBKKl"
      },
      "source": [
        "## Task 4\n",
        "\n",
        "Add the torch expressions to the following code to calculate the least squares error using torch and check that it calculates the same value as the numpy version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mkZTQYQBKKl"
      },
      "outputs": [],
      "source": [
        "# Initial values for optimisation..\n",
        "w_initial_guess = 1.5\n",
        "c_initial_guess = 0.5\n",
        "\n",
        "# Constants to hold the training data..\n",
        "t_x_train = torch.tensor(x_train)\n",
        "t_y_train = torch.tensor(y_train)\n",
        "\n",
        "# Variables to hold w and c\n",
        "t_w = torch.tensor(w_initial_guess, \n",
        "                  dtype=torch.float64, \n",
        "                  requires_grad=True)\n",
        "t_c = torch.tensor(c_initial_guess, \n",
        "                  dtype=torch.float64, \n",
        "                  requires_grad=True)\n",
        "\n",
        "\n",
        "def calculate_least_squares_error_torch(t_x_train, t_y_train, t_w, t_c):\n",
        "\n",
        "    # Add your code here to calculate t_least_squares_error..\n",
        "    # ************************************************************\n",
        "    # ...\n",
        "    \n",
        "    \n",
        "    \n",
        "    return t_least_squares_error\n",
        "\n",
        "\n",
        "t_least_squares_error = calculate_least_squares_error_torch(t_x_train, \n",
        "                                                            t_y_train, \n",
        "                                                            t_w, \n",
        "                                                            t_c)\n",
        "\n",
        "# Check with the result from your previous function..\n",
        "numpy_result = least_squares_error(x_train, y_train, \n",
        "                                   w=w_initial_guess, \n",
        "                                   c=c_initial_guess)\n",
        "\n",
        "print('Torch least squares error = ', t_least_squares_error)\n",
        "print('numpy least squares error = ', numpy_result)\n",
        "\n",
        "# This should pass if they are the same to nummerical precision!\n",
        "assert(np.isclose(t_least_squares_error.item(), numpy_result))\n",
        "\n",
        "print('Assertion passed - same result to nummerical precision!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0l1IyXzBKKl"
      },
      "source": [
        "## Task 5\n",
        "\n",
        "Check that the gradients from torch are correct by writing numpy code to calculate the value of the derivatives from the analytic expressions (derived previously):\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial E}{\\partial w} \n",
        "    &= \\sum_{i=0}^{N-1} 2 x_i (w x_i + c - y_i) \\\\\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial E}{\\partial c} \n",
        "    &= \\sum_{i=0}^{N-1} 2 (w x_i + c - y_i) \\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzJPhcTMBKKm"
      },
      "outputs": [],
      "source": [
        "# Clear and gradient data and run the backwards pass to calculate the\n",
        "# gradients in pytorch (as in the pytorch example notebook)\n",
        "if t_w.grad is not None:\n",
        "    t_w.grad.data.zero_()\n",
        "if t_c.grad is not None:\n",
        "    t_c.grad.data.zero_()\n",
        "\n",
        "# Note: In this instance we use retain_graph for the purposes of debugging\n",
        "# (i.e. we might use the backward pass multiple times)\n",
        "t_least_squares_error.backward(retain_graph=True)\n",
        "\n",
        "print('Torch gradient wrt w = ', t_w.grad)\n",
        "print('Torch gradient wrt c = ', t_c.grad)\n",
        "\n",
        "def calc_gradients_for_least_squares(x, y, w, c):\n",
        "    # Add your code to evalute the partial derivatives here\n",
        "    # ************************************************************\n",
        "    # ...\n",
        "    \n",
        "    \n",
        "    \n",
        "    return grad_w, grad_c\n",
        "\n",
        "numpy_grad_w, numpy_grad_c = calc_gradients_for_least_squares(x_train, \n",
        "                                                              y_train, \n",
        "                                                              w_initial_guess, \n",
        "                                                              c_initial_guess)\n",
        "\n",
        "print('Analytic gradient wrt w = ', numpy_grad_w)\n",
        "print('Analytic gradient wrt c = ', numpy_grad_c)\n",
        "\n",
        "# This should pass if they are the same to nummerical precision!\n",
        "assert(np.isclose(t_w.grad.item(), numpy_grad_w))\n",
        "assert(np.isclose(t_c.grad.item(), numpy_grad_c))\n",
        "\n",
        "print('Assertion passed - same result to nummerical precision!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oTSm-kLBKKm"
      },
      "source": [
        "## Gradient Descent in PyTorch\n",
        "\n",
        "If all has gone well, the gradients should be the same and you can use torch to find the solution to the optimisation.\n",
        "\n",
        "**Run the following block of code to see the torch optimisation running with your least square error function from above!**\n",
        "\n",
        "*Aside: Investigate what happens as you change the `learning_rate` parameter as well as the `num_iterations`. Can we guarantee that torch will always return the same result as the analytic solution? What might be happening if not?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxLLBseMBKKm"
      },
      "outputs": [],
      "source": [
        "# Create a gradient descent optimiser that uses a\n",
        "# certain step size (learning_rate)..\n",
        "learning_rate = 1.0e-3\n",
        "\n",
        "# Variables to hold w and c, initialised appropriately\n",
        "t_w = torch.tensor(w_initial_guess, \n",
        "                  dtype=torch.float64, \n",
        "                  requires_grad=True)\n",
        "t_c = torch.tensor(c_initial_guess, \n",
        "                  dtype=torch.float64, \n",
        "                  requires_grad=True)\n",
        "\n",
        "# We want to optimise wrt w and c\n",
        "vars_to_optimise = [t_w, t_c]\n",
        "\n",
        "optimizer = torch.optim.SGD(params=vars_to_optimise, lr=learning_rate)\n",
        "\n",
        "# Number of iterations to perform\n",
        "num_iterations = 15\n",
        "\n",
        "for iteration in range(num_iterations + 1):\n",
        "    # Perform the forward pass, calculate the error..\n",
        "    t_objective = calculate_least_squares_error_torch(t_x_train, \n",
        "                                                      t_y_train, \n",
        "                                                      t_w, \n",
        "                                                      t_c)\n",
        "\n",
        "    # Clear the gradients..\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform the backwards pass to calculate the gradients..\n",
        "    t_objective.backward()\n",
        "\n",
        "    # Update the parameters via the optimiser..\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print out the current values\n",
        "    print('iter %4d, E(w,c) = %0.3f' % \n",
        "                (iteration + 1, t_objective.item()))\n",
        "    \n",
        "print('\\nAfter torch optimisation:')\n",
        "print('Torch w = ', t_w)\n",
        "print('Torch c = ', t_c)\n",
        "\n",
        "print('\\nAnalytic solution:')\n",
        "print('Analytic w = ', w_opt)\n",
        "print('Analytic c = ', c_opt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNsN-o9OBKKn"
      },
      "source": [
        "## Task 6\n",
        "\n",
        "You are now going to implement your own version of gradient descent (the process that torch is providing to perform optimisation). The good news is that you already have the numpy functions to calculate both the least squares error (`least_squares_error`) and the gradients wrt the parameters (`calc_gradients_for_least_squares`).\n",
        "\n",
        "Starting from the same initial values as torch (`w_initial_guess`, `c_initial_guess`) write an iterative algorithm for gradient descent. At each iteration it should perform the following steps:\n",
        "\n",
        "- Evaluate and save the squared error for the current parameters\n",
        "- Calculate the gradients wrt w and c for the current parameters\n",
        "- Update the parameters for w and c my moving in the direction of the negative current proportional to the current step size\n",
        "\n",
        "**The code below should run with these three additions. Once they are working, move on to the following.**\n",
        "\n",
        "Under this algorithm, the squared error should decrease at each iteration. If it is not decreasing then either there is a bug in the code (!) or the step size is too large. Add a check that makes sure the squared error always decreases and if it fails to decrease, decrease the step size and run the iteration again.\n",
        "\n",
        "**Try running this code starting with `current_step_size = 0.1`.**\n",
        "\n",
        "You can improve your answer by checking to see if you should stop iterating. If the change in the squared error between successive iterations is very small then one of the following is true. Either the step size is too small (not changing the parameters sufficiently) or the values have converged to their optimal values. If you cannot find a step size that creates a deacrease in the squared error then you have probably converged and can stop performing iterations.\n",
        "\n",
        "*Hint: you can exit a `for` loop early with the `break` command.*\n",
        "\n",
        "**Try running this code starting with `num_iterations = 200`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMVdZ-MGBKKn"
      },
      "outputs": [],
      "source": [
        "# Keep track of parameter values over iterations..\n",
        "w_current = w_initial_guess\n",
        "c_current = c_initial_guess\n",
        "\n",
        "# Keep track of the error..\n",
        "E_current = least_squares_error(x_train, y_train, w_current, c_current)\n",
        "\n",
        "# Keep track of the step size..\n",
        "current_step_size = 0.1\n",
        "\n",
        "num_iterations = 200\n",
        "\n",
        "converge_threshold = 1e-13\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    \n",
        "    # Add code to evaluate the gradients..\n",
        "    # ************************************************************\n",
        "    # ...\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Add code to take a step in the direction of the negative\n",
        "    # gradient proportional to the step size..\n",
        "    # ************************************************************\n",
        "    # ...\n",
        "\n",
        "    w_new = \n",
        "    c_new = \n",
        "    \n",
        "    # Add code to evaluate and remember the squared error..\n",
        "    # ************************************************************\n",
        "    # ...\n",
        "\n",
        "    E_new = \n",
        "\n",
        "    # Add code to check that error is decreasing and reduce step\n",
        "    # size if not..\n",
        "    # ************************************************************\n",
        "    # ...\n",
        "\n",
        "    \n",
        "\n",
        "    # Add code to check for convergence and terminate\n",
        "    # the loop if converged..\n",
        "    # ************************************************************\n",
        "    # ...\n",
        "    \n",
        "    \n",
        "    \n",
        "    print('iteration %4d, E = %f, w = %f, c = %f' % \n",
        "      (iteration, E_new, w_new, c_new))\n",
        "    \n",
        "    if converged:\n",
        "        print('Converged!')\n",
        "        break\n",
        "    \n",
        "    # Take the step\n",
        "    w_current = w_new\n",
        "    c_current = c_new\n",
        "    E_current = E_new\n",
        "\n",
        "print('\\nAfter gradient descent optimisation:')\n",
        "print('Optimised w = ', w_current)\n",
        "print('Optimised c = ', c_current)\n",
        "\n",
        "print('\\nAnalytic solution:')\n",
        "print('Analytic w = ', w_opt)\n",
        "print('Analytic c = ', c_opt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Task: Create a PyTorch Module\n",
        "\n",
        "We can create a custom module for our model so that we can train using a standard pytorch workflow (as in the pytorch example notebook). \n",
        "\n"
      ],
      "metadata": {
        "id": "mfB4nf7MGOE-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eeKvk_S8BKKo"
      },
      "outputs": [],
      "source": [
        "# Create a gradient descent optimiser that uses a\n",
        "# certain step size (learning_rate)..\n",
        "learning_rate = 1.0e-3\n",
        "\n",
        "# Create our own class derived from the torch module..\n",
        "class LinearRegressionModel(torch.nn.Module):\n",
        "    # We must initialise our model - we specify our initial guesses for the \n",
        "    # parameters..\n",
        "    def __init__(self, w_initial_guess, c_initial_guess):\n",
        "        # Call the constructor for the torch.nn.Module super class..\n",
        "        super().__init__()\n",
        "\n",
        "        # We use the Parameter class (rather than tensors) for the module\n",
        "        # but these behave in the same way..\n",
        "        self.t_w = torch.nn.Parameter(torch.tensor(w_initial_guess))\n",
        "        self.t_c = torch.nn.Parameter(torch.tensor(c_initial_guess))\n",
        "\n",
        "    # This defines the forward operation on some data passed in \n",
        "    def forward(self, t_x_train, t_y_train):\n",
        "        # Calculate the error using our function defined above and the\n",
        "        # data passed in. Remember to use the parameters from the class..\n",
        "        t_objective = calculate_least_squares_error_torch(t_x_train, \n",
        "                                                          t_y_train, \n",
        "                                                          self.t_w, \n",
        "                                                          self.t_c)\n",
        "\n",
        "        return t_objective \n",
        "\n",
        "# Create our model - it will initialise the paramters appropriately\n",
        "linear_regression_model = LinearRegressionModel(w_initial_guess, c_initial_guess)\n",
        "# Put the model in training mode..\n",
        "linear_regression_model.train()\n",
        "\n",
        "optimizer = torch.optim.SGD(params=linear_regression_model.parameters(), \n",
        "                            lr=learning_rate)\n",
        "\n",
        "# Number of iterations to perform\n",
        "num_iterations = 15\n",
        "\n",
        "for iteration in range(num_iterations + 1):\n",
        "    # Perform the forward pass, calculate the error on the specified data..\n",
        "    t_objective = linear_regression_model(t_x_train, t_y_train)\n",
        "\n",
        "    # Clear the gradients..\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform the backwards pass to calculate the gradients..\n",
        "    t_objective.backward()\n",
        "\n",
        "    # Update the parameters via the optimiser..\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print out the current values\n",
        "    print('iter %4d, E(w,c) = %0.3f' % \n",
        "                (iteration + 1, t_objective.item()))\n",
        "    \n",
        "print('\\nAfter torch optimisation:')\n",
        "print('Torch w = ', t_w)\n",
        "print('Torch c = ', t_c)\n",
        "\n",
        "print('\\nAnalytic solution:')\n",
        "print('Analytic w = ', w_opt)\n",
        "print('Analytic c = ', c_opt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further tasks:\n",
        "\n",
        "- Add a prediction function to the class above so that you can use the trained model to run predictions on new test data - you can use this to plot the output of the model after training.\n",
        "- Load up a different source of data to test your model on.\n",
        "- Add minibatching to allow you to run each iteration on a subset of the data.\n",
        "- Produce a new custom module that implements a fully connected neural network (or MLP) to produce predictions on your new dataset. You can run through the torch [neural networks tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html)."
      ],
      "metadata": {
        "id": "0kFOsW_jIZHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bND4fhkDHsVb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "PyTorch Linear Regression Lab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}